{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "_uuid": "56b80176f1af4f1b90a1acfd7b0ecf11ab782d4c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string, re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, Embedding, Dropout\n",
    "from keras import metrics\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (130612, 3)\n",
      "Test shape :  (56370, 2)\n"
     ]
    }
   ],
   "source": [
    "train_raw_df = pd.read_csv(\"input/train.csv\")\n",
    "test_df = pd.read_csv(\"input/test.csv\")\n",
    "train_n = len(train_df)\n",
    "test_n = len(test_df)\n",
    "print(\"Train shape : \",train_df.shape)\n",
    "print(\"Test shape : \",test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "_uuid": "1e1424a093fa82afe61b7b55a6587a3e64145255"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (65306, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>737723</th>\n",
       "      <td>907a93d2f27d74b9a224</td>\n",
       "      <td>When did London lose its manners?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         qid                      question_text  target\n",
       "737723  907a93d2f27d74b9a224  When did London lose its manners?       1"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_raw_df.sample(frac=0.05)\n",
    "print(\"Train shape : \",train_df.shape)\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "_uuid": "43c12d450ed66e081b7aa5e486a656482e80914c"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stopset = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "_uuid": "2f3db298d15bd0cd6eb3ed6d7d5b488e016ad432"
   },
   "outputs": [],
   "source": [
    "def process(txt):\n",
    "    tokens = []\n",
    "    for token in wordpunct_tokenize(txt):\n",
    "        if token.isdigit():\n",
    "            continue\n",
    "        if all(char in string.punctuation for char in token):\n",
    "            continue\n",
    "        \n",
    "        token = token.lower()\n",
    "        token = token.strip()  # Strip whitespace and other punctuations\n",
    "        token = token.strip('_')  # remove _ if any\n",
    "        token = token.strip('*')\n",
    "        if token in stopset:\n",
    "            continue\n",
    "        tokens.append(token)\n",
    "        lemmatizer.lemmatize(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "_uuid": "4885b08cca7bb1c44c15febbdc8d4194998cd17e"
   },
   "outputs": [],
   "source": [
    "train_df['p_txt'] = train_df['question_text'].apply(process)\n",
    "test_df['p_txt'] = test_df['question_text'].apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "_uuid": "fbba0bae21578708f62c6725ed1f0cde32d733d1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>p_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>737723</th>\n",
       "      <td>907a93d2f27d74b9a224</td>\n",
       "      <td>When did London lose its manners?</td>\n",
       "      <td>1</td>\n",
       "      <td>[london, lose, manners]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         qid                      question_text  target                    p_txt\n",
       "737723  907a93d2f27d74b9a224  When did London lose its manners?       1  [london, lose, manners]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "_uuid": "35bdd3ef78726d0b42fb827439810578f9da4edf"
   },
   "outputs": [],
   "source": [
    "train_df['p_txtcn'] = train_df['p_txt'].apply(lambda tokens: ' '.join(str(v) for v in tokens))\n",
    "test_df['p_txtcn'] = test_df['p_txt'].apply(lambda tokens: ' '.join(str(v) for v in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "_uuid": "7a9f0f94bcfb6224460eec9bf1d980056d1eb702"
   },
   "outputs": [],
   "source": [
    "no_features = 1000\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "_uuid": "27f60507665d70b532a04ab6f479af1b2aeccf24"
   },
   "outputs": [],
   "source": [
    "def vectorize(txt, vectorizer):\n",
    "    X = vectorizer.fit_transform(txt)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    return X, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "_uuid": "a575684c07286add20669c7941bac4a04d25b684"
   },
   "outputs": [],
   "source": [
    "X_tf, tf_feature_names = vectorize(train_df['p_txtcn'], tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "_uuid": "1f7b0498071a24169514342d5e6a7a7937763269"
   },
   "outputs": [],
   "source": [
    "no_topics = 10\n",
    "num_iter = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "_uuid": "97698464aff47dd1c030ce2c80f05a481a3ad051"
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=num_iter, learning_method='online', learning_offset=50.,random_state=9, evaluate_every=100).fit(X_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "_uuid": "d61ed404a47066640545b863ad56ed43c12cc65a"
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features = []\n",
    "        for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
    "            if topic[i] >  1.0:\n",
    "                top_features.append((feature_names[i], topic[i]))\n",
    "            else:\n",
    "                continue        \n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\", \".join([str(val[0])+\": \"+\"%.2f\" % val[1] for val in top_features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "best: 927.17, make: 557.96, life: 536.23, way: 480.90, better: 378.16, country: 287.58, would: 287.41, year: 269.22, long: 249.43, old: 248.40, say: 245.31, day: 215.46, live: 212.35, new: 201.18, years: 198.91, girl: 195.31, known: 143.50, eat: 140.83, friend: 138.39, another: 129.41, win: 127.12, found: 125.32, market: 115.66, believe: 110.35, less: 110.25, weight: 106.89, fight: 99.21, girlfriend: 94.76, die: 93.87, culture: 93.14, project: 91.62, term: 90.76, oil: 88.63, people: 88.29, modi: 84.46, talk: 83.82, skin: 82.17, case: 80.55, prevent: 78.88, modern: 78.05, rate: 76.65, exist: 76.15, control: 76.01, run: 75.63, sleep: 75.05, good: 74.83, price: 74.57, eating: 73.95, vs: 73.26, explain: 72.98\n",
      "Topic 1:\n",
      "become: 446.13, work: 432.94, take: 272.38, engineering: 246.62, anyone: 245.40, college: 241.32, without: 226.44, go: 224.12, learn: 203.51, getting: 193.47, get: 187.60, university: 185.89, look: 183.53, company: 179.51, like: 175.06, career: 170.12, students: 165.72, science: 162.34, questions: 158.79, never: 152.81, course: 145.39, kind: 141.23, friends: 139.77, facebook: 135.60, degree: 133.98, canada: 133.94, called: 133.53, affect: 133.50, relationship: 125.78, self: 121.45, woman: 119.89, jobs: 118.83, writing: 118.51, space: 118.42, great: 117.31, technology: 116.84, data: 115.28, international: 111.01, popular: 100.74, hair: 100.24, salary: 99.11, job: 98.44, uk: 97.41, male: 88.93, theory: 87.33, said: 86.96, quora: 84.99, physics: 84.80, away: 84.00, everything: 83.13\n",
      "Topic 2:\n",
      "many: 467.47, one: 348.18, start: 319.84, different: 318.00, love: 258.71, stop: 230.23, person: 217.60, much: 210.00, even: 206.62, meaning: 142.18, today: 138.67, worth: 137.16, well: 131.12, read: 130.75, guy: 129.94, delhi: 123.53, music: 122.78, medical: 121.23, question: 120.96, best: 111.71, youtube: 110.94, average: 108.46, anything: 107.26, mass: 106.92, answer: 103.33, education: 102.61, favorite: 101.25, required: 98.59, female: 98.10, energy: 97.76, companies: 97.42, india: 97.10, media: 96.99, iphone: 96.09, liberals: 92.48, software: 91.69, people: 90.27, political: 89.79, gay: 87.99, describe: 86.11, would: 86.10, star: 81.09, police: 80.39, per: 80.02, period: 79.24, news: 76.82, kids: 73.67, want: 73.03, reading: 72.84, network: 71.42\n",
      "Topic 3:\n",
      "time: 480.28, find: 409.80, world: 393.54, people: 326.66, book: 273.23, could: 266.64, number: 193.48, online: 189.39, going: 183.49, best: 174.56, tell: 170.46, write: 165.83, social: 163.99, sex: 163.57, website: 159.38, learning: 158.18, car: 146.21, cause: 138.39, test: 129.98, earth: 126.72, point: 121.98, tips: 120.95, last: 117.94, game: 114.48, usa: 113.69, safe: 110.48, engineer: 108.25, form: 103.54, short: 103.40, bank: 102.95, good: 101.50, military: 99.41, uses: 97.73, application: 96.28, times: 94.39, machine: 92.88, consider: 92.63, opinion: 92.28, face: 89.33, full: 85.42, pain: 84.90, speed: 84.66, red: 84.49, starting: 81.58, first: 81.13, second: 80.04, living: 77.31, sound: 75.54, many: 74.37, group: 73.16\n",
      "Topic 4:\n",
      "use: 518.31, india: 472.71, business: 254.58, help: 244.11, using: 234.27, buy: 229.88, want: 216.36, keep: 193.33, ways: 192.14, free: 189.50, government: 170.12, war: 168.66, big: 163.48, phone: 160.22, place: 159.39, man: 158.03, non: 151.80, examples: 150.68, power: 149.48, app: 145.95, create: 121.87, deal: 119.41, apply: 115.11, improve: 112.86, pay: 111.03, benefits: 110.59, would: 106.84, research: 105.56, good: 105.24, public: 104.33, one: 104.08, making: 103.56, australia: 103.04, level: 102.53, build: 102.20, south: 101.79, open: 97.96, post: 96.37, legal: 94.78, biggest: 94.71, android: 94.20, role: 89.77, society: 89.04, tax: 87.24, japanese: 87.21, development: 86.42, looking: 85.07, amazon: 84.30, death: 84.05, area: 83.63\n",
      "Topic 5:\n",
      "women: 276.07, quora: 263.76, money: 250.25, give: 221.19, thing: 213.54, important: 207.36, men: 201.24, first: 199.18, black: 196.67, major: 179.62, history: 164.34, every: 156.19, food: 151.60, always: 144.98, value: 133.47, choose: 126.51, word: 118.09, end: 117.87, join: 115.81, given: 112.81, increase: 110.54, light: 108.30, others: 107.52, air: 104.24, taking: 103.97, indians: 99.39, often: 99.14, date: 99.13, like: 98.53, single: 96.35, lose: 95.71, math: 94.71, internet: 94.05, boyfriend: 92.60, enough: 91.66, problems: 91.33, force: 90.50, best: 89.81, turn: 88.82, card: 87.64, industry: 86.73, amount: 85.99, guys: 79.20, programming: 79.12, sites: 77.90, idea: 77.85, add: 76.83, office: 76.75, blood: 76.56, people: 75.84\n",
      "Topic 6:\n",
      "indian: 310.62, change: 231.16, causes: 169.64, girls: 165.35, white: 162.79, computer: 154.70, age: 145.77, system: 143.58, days: 143.39, god: 139.47, process: 128.93, prepare: 123.07, muslims: 121.45, video: 120.59, law: 119.96, type: 116.93, service: 116.66, since: 113.26, support: 110.53, children: 105.85, design: 104.91, humans: 99.63, wear: 93.79, field: 91.39, mobile: 90.26, night: 86.46, similar: 85.02, code: 84.03, universe: 80.46, marriage: 80.26, russia: 78.54, brain: 77.02, islam: 75.24, factors: 74.12, allowed: 73.02, ideas: 72.08, foreign: 72.08, father: 71.63, ex: 71.50, line: 70.81, function: 69.86, differences: 69.09, large: 68.98, upsc: 68.26, determine: 68.12, liberal: 66.64, drive: 66.48, names: 66.28, half: 65.38, contact: 65.18\n",
      "Topic 7:\n",
      "get: 793.44, someone: 465.41, possible: 339.43, used: 336.64, need: 230.34, student: 189.94, books: 178.37, class: 178.08, american: 177.33, something: 177.09, state: 175.36, top: 160.84, parents: 154.22, language: 142.72, rank: 136.15, house: 131.18, human: 129.20, actually: 128.18, got: 127.96, hard: 125.63, considered: 125.27, movies: 121.40, call: 121.00, play: 116.92, order: 115.51, score: 115.25, purpose: 113.71, jee: 113.48, ask: 110.40, small: 109.18, tv: 105.56, put: 104.27, visit: 103.96, health: 103.91, iit: 103.54, based: 102.72, available: 102.50, marks: 101.25, general: 100.21, one: 99.75, admission: 99.55, program: 98.06, seen: 96.21, management: 96.06, watch: 96.03, left: 93.16, sell: 93.11, move: 92.60, paper: 90.97, series: 90.50\n",
      "Topic 8:\n",
      "think: 596.04, ever: 447.74, difference: 399.47, good: 389.57, mean: 368.01, know: 319.90, really: 264.89, school: 253.40, happen: 246.12, made: 241.15, bad: 221.19, name: 212.48, high: 206.42, back: 198.49, two: 198.36, see: 186.49, job: 182.31, experience: 181.54, account: 174.67, working: 156.56, happens: 151.08, future: 149.66, body: 139.76, movie: 138.26, get: 137.87, common: 133.48, family: 131.52, people: 128.57, main: 126.78, study: 123.61, would: 123.57, next: 114.10, marketing: 112.35, one: 112.11, mind: 100.89, download: 100.61, reason: 98.00, able: 96.23, training: 92.95, person: 88.16, germany: 87.98, muslim: 86.17, lost: 86.11, british: 85.28, lot: 85.24, difficult: 83.17, follow: 82.09, mechanical: 80.85, remove: 78.82, grow: 74.18\n",
      "Topic 9:\n",
      "like: 610.53, feel: 382.31, trump: 283.95, us: 248.12, still: 245.27, things: 240.32, water: 229.24, people: 200.33, china: 196.50, countries: 184.98, right: 182.47, come: 179.87, home: 178.89, chinese: 177.02, real: 176.49, true: 168.16, normal: 156.27, english: 154.16, america: 150.39, states: 142.11, hate: 141.55, cost: 140.74, child: 132.60, makes: 130.47, part: 129.93, around: 129.54, problem: 128.64, done: 127.72, google: 126.44, show: 125.89, current: 124.75, wrong: 118.98, americans: 116.46, pakistan: 113.73, would: 113.27, india: 112.43, president: 109.98, united: 107.09, natural: 102.90, city: 102.02, dog: 100.61, story: 99.15, stay: 98.57, north: 96.85, effective: 96.69, characteristics: 93.35, types: 92.53, song: 87.62, donald: 85.59, kill: 85.59\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda, tf_feature_names, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lda = lda.transform(X_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_lda[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>idxmax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.029061</td>\n",
       "      <td>0.503562</td>\n",
       "      <td>0.029061</td>\n",
       "      <td>0.029061</td>\n",
       "      <td>0.029061</td>\n",
       "      <td>0.029061</td>\n",
       "      <td>0.133664</td>\n",
       "      <td>0.029061</td>\n",
       "      <td>0.029061</td>\n",
       "      <td>0.159347</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6         7         8         9  idxmax\n",
       "0  0.050000  0.050000  0.050000  0.050000  0.050000  0.550000  0.050000  0.050000  0.050000  0.050000       5\n",
       "1  0.029061  0.503562  0.029061  0.029061  0.029061  0.029061  0.133664  0.029061  0.029061  0.159347       1"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_lda_df = pd.DataFrame(X_lda)\n",
    "X_lda_df[\"idxmax\"] = X_lda_df.idxmax(axis=1)\n",
    "X_lda_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = train_df[\"target\"]\n",
    "y_lda = X_lda_df[\"idxmax\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9865 6852 5490 5461 6203 4329 4459 6899 5707 6023]\n",
      " [ 478  316  434  265  314  365  525  238  321  762]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_true, y_lda)[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying Deep Learning with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_get_sequences(df, vocab_size):\n",
    "    tokenizer = Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(df['p_txt'])\n",
    "    sequences = tokenizer.texts_to_sequences(df['p_txt'])\n",
    "    return sequences, tokenizer, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences(df, tokenizer):\n",
    "    return tokenizer.texts_to_sequences(df['p_txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40712 unique tokens.\n",
      "Tokens avg 3.3403515756592044 and std 2.4788018235639098\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1000\n",
    "sequences, tokenizer, vocab_size = fit_get_sequences(train_df, vocab_size)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "avg = sum(map(len, sequences)) / len(sequences)\n",
    "std = np.sqrt(sum(map(lambda x: (len(x) - avg) ** 2, sequences)) / len(sequences))\n",
    "print(\"Tokens avg {} and std {}\".format(avg, std))\n",
    "\n",
    "max_length = 100\n",
    "X = pad_sequences(sequences, maxlen=max_length)\n",
    "X_test = pad_sequences(get_sequences(test_df, tokenizer), maxlen=max_length)\n",
    "\n",
    "y = to_categorical(np.asarray(train_df['target']))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (65306, 100)\n",
      "Shape of labels: (65306, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data:', X.shape)\n",
    "print('Shape of labels:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(vocab_size, word_index):\n",
    "    EMBEDDING_FILE = \"/Users/saurabh/Downloads/glove.6B/\" + \"glove.6B.100d.txt\"\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join(EMBEDDING_FILE))\n",
    "    # In the dataset, each line represents a new word embedding\n",
    "    # The line starts with the word and the embedding values follow\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "    f.close()\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean = all_embs.mean()\n",
    "    emb_std = all_embs.std()\n",
    "    print(emb_mean, emb_std)\n",
    "    embedding_dim = 100\n",
    "    nb_words = min(vocab_size, len(word_index))  # How many words are there actually\n",
    "    # Create a random matrix with the same mean and std as the embeddings\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_dim))\n",
    "    # The vectors need to be in the same position as their index.\n",
    "    # Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "    # Loop over all words in the word index\n",
    "    for word, i in word_index.items():\n",
    "        # If we are above the amount of words we want to use we do nothing\n",
    "        if i >= vocab_size:\n",
    "            break\n",
    "        # Get the embedding vector for the word\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        # If there is an embedding vector, put it in the embedding matrix\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_dim, embedding_matrix    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004451992 0.4081574\n"
     ]
    }
   ],
   "source": [
    "embedding_dim, embedding_matrix = generate_embeddings(vocab_size, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(embedding_dim, embedding_matrix, max_length, vocab_size):\n",
    "    model = Sequential()\n",
    "    # model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embedding_matrix], trainable=False))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(32, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                  metrics=[metrics.mae, metrics.categorical_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 100, 100)          100000    \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 100, 32)           17024     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 100, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 125,410\n",
      "Trainable params: 25,410\n",
      "Non-trainable params: 100,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = generate_model(embedding_dim, embedding_matrix, max_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 47019 samples, validate on 5225 samples\n",
      "Epoch 1/5\n",
      "47019/47019 [==============================] - 50s 1ms/step - loss: 0.2718 - mean_absolute_error: 0.1563 - categorical_accuracy: 0.9277 - val_loss: 0.2262 - val_mean_absolute_error: 0.1217 - val_categorical_accuracy: 0.9361\n",
      "Epoch 2/5\n",
      "47019/47019 [==============================] - 50s 1ms/step - loss: 0.2163 - mean_absolute_error: 0.1135 - categorical_accuracy: 0.9390 - val_loss: 0.2140 - val_mean_absolute_error: 0.1051 - val_categorical_accuracy: 0.9361\n",
      "Epoch 3/5\n",
      "47019/47019 [==============================] - 52s 1ms/step - loss: 0.1985 - mean_absolute_error: 0.1049 - categorical_accuracy: 0.9394 - val_loss: 0.1902 - val_mean_absolute_error: 0.0908 - val_categorical_accuracy: 0.9378\n",
      "Epoch 4/5\n",
      "47019/47019 [==============================] - 48s 1ms/step - loss: 0.1818 - mean_absolute_error: 0.0967 - categorical_accuracy: 0.9405 - val_loss: 0.1785 - val_mean_absolute_error: 0.0900 - val_categorical_accuracy: 0.9391\n",
      "Epoch 5/5\n",
      "47019/47019 [==============================] - 49s 1ms/step - loss: 0.1752 - mean_absolute_error: 0.0943 - categorical_accuracy: 0.9416 - val_loss: 0.1731 - val_mean_absolute_error: 0.0982 - val_categorical_accuracy: 0.9397\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12f53ac18>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=5, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_matrix(y_true, y_pred, num_classes=2):\n",
    "    cm = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    print(cm)\n",
    "    cm_np = np.asarray(cm)\n",
    "    TP = np.diag(cm_np)\n",
    "    FP = np.sum(cm, axis=0) - TP\n",
    "    FN = np.sum(cm, axis=1) - TP\n",
    "    TN = []\n",
    "    for i in range(num_classes):\n",
    "        temp = np.delete(cm, i, 0)\n",
    "        temp = np.delete(temp, i, 1)\n",
    "        TN.append(sum(sum(temp)))\n",
    "    prec = TP / (TP + FP)\n",
    "    rec = TP / (TP + FN)\n",
    "    acc = (TP + TN) / (TP + FP + TN + FN)\n",
    "    f1 = 2 * prec * rec / (prec + rec)\n",
    "\n",
    "    print(\"accuracy\", acc)\n",
    "    print(\"precision\", prec)\n",
    "    print(\"recall\", rec)\n",
    "    print(\"f1\", f1)\n",
    "    return prec, rec, acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12149    96]\n",
      " [  709   108]]\n",
      "accuracy [0.93837085 0.93837085]\n",
      "precision [0.94485923 0.52941176]\n",
      "recall [0.99216007 0.13219094]\n",
      "f1 [0.96793212 0.2115573 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.94485923, 0.52941176]),\n",
       " array([0.99216007, 0.13219094]),\n",
       " array([0.93837085, 0.93837085]),\n",
       " array([0.96793212, 0.2115573 ]))"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "c_matrix(y_test, y_pred, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
