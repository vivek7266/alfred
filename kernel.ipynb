{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['embeddings', 'train.csv', 'sample_submission.csv', 'test.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "56b80176f1af4f1b90a1acfd7b0ecf11ab782d4c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport string, re\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.metrics import confusion_matrix\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import wordpunct_tokenize\nfrom nltk.stem import WordNetLemmatizer",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e16fc01dc31e4d838a86c538cae65bf2554d68f"
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Activation, Embedding, Dropout\nfrom keras import metrics\nfrom keras.utils import to_categorical",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c54a1a3e0fb6a29c80cd644b5c685b9be26ac252"
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6835db2d355393ba6124fcf8a7cf8c3893e75a88"
      },
      "cell_type": "code",
      "source": "pd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_raw_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\ntrain_df = train_raw_df\n# train_df = train_raw_df.sample(frac=0.5)\n# train_df = train_df.append(train_raw_df[train_raw_df['target'] == 1])\ntrain_n = len(train_df)\ntest_n = len(test_df)\nprint(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train shape :  (1306122, 3)\nTest shape :  (56370, 2)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6e9ad7e57b7e9a9ef61355f78e2991402b4502b4"
      },
      "cell_type": "code",
      "source": "train_df.groupby('target').count()",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "            qid  question_text\ntarget                        \n0       1225312        1225312\n1         80810          80810",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>question_text</th>\n    </tr>\n    <tr>\n      <th>target</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1225312</td>\n      <td>1225312</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>80810</td>\n      <td>80810</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "617609719444fe6c761d022e10fb6d8fcde9a400"
      },
      "cell_type": "code",
      "source": "train_df.head(1)\ntest_df.iloc[594,:][1]",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "'Has anyone gotten banned on Quora for being under 13?'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "43c12d450ed66e081b7aa5e486a656482e80914c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "lemmatizer = WordNetLemmatizer()\nstopset = set(stopwords.words('english'))",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2f3db298d15bd0cd6eb3ed6d7d5b488e016ad432",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def process(txt):\n    tokens = []\n    for token in wordpunct_tokenize(txt):\n        if token.isdigit():\n            continue\n        if all(char in string.punctuation for char in token):\n            continue\n        \n        token = token.lower()\n        token = token.strip()  # Strip whitespace and other punctuations\n        token = token.strip('_')  # remove _ if any\n        token = token.strip('*')\n        if token in stopset:\n            continue\n        tokens.append(token)\n        lemmatizer.lemmatize(token)\n    return tokens",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4885b08cca7bb1c44c15febbdc8d4194998cd17e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_df['p_txt'] = train_df['question_text'].apply(process)\ntest_df['p_txt'] = test_df['question_text'].apply(process)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fbba0bae21578708f62c6725ed1f0cde32d733d1",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "35bdd3ef78726d0b42fb827439810578f9da4edf",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_df['p_txtcn'] = train_df['p_txt'].apply(lambda tokens: ' '.join(str(v) for v in tokens))\ntest_df['p_txtcn'] = test_df['p_txt'].apply(lambda tokens: ' '.join(str(v) for v in tokens))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7a9f0f94bcfb6224460eec9bf1d980056d1eb702",
        "trusted": true
      },
      "cell_type": "code",
      "source": "no_features = 5000\ntfidf_vectorizer = TfidfVectorizer(max_features=no_features)\n# tf_vectorizer = CountVectorizer(max_features=no_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "27f60507665d70b532a04ab6f479af1b2aeccf24",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def vectorize(txt, vectorizer):\n    X = vectorizer.fit_transform(txt)\n    feature_names = vectorizer.get_feature_names()\n    return X, feature_names, vectorizer",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a575684c07286add20669c7941bac4a04d25b684",
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_tf, tf_feature_names, vectorizer = vectorize(train_df['p_txtcn'], tfidf_vectorizer)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1f7b0498071a24169514342d5e6a7a7937763269",
        "trusted": true
      },
      "cell_type": "code",
      "source": "no_topics = 25\nnum_iter = 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "97698464aff47dd1c030ce2c80f05a481a3ad051",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# lda = LatentDirichletAllocation(n_components=no_topics, max_iter=num_iter, learning_method='online', learning_offset=50.,random_state=9, evaluate_every=100).fit(X_tf)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d61ed404a47066640545b863ad56ed43c12cc65a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def display_topics(model, feature_names, no_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        top_features = []\n        for i in topic.argsort()[:-no_top_words - 1:-1]:\n            if topic[i] >  1.0:\n                top_features.append((feature_names[i], topic[i]))\n            else:\n                continue        \n        print(\"Topic %d:\" % (topic_idx))\n        print(\", \".join([str(val[0])+\": \"+\"%.2f\" % val[1] for val in top_features]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "00ed3c18da2bbdf307de9b30607320e4aeae3bc6",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "# display_topics(lda, tf_feature_names, 5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "76b030e9f0ce61db12ef3bbd740a5e53d135d13d"
      },
      "cell_type": "code",
      "source": "# X_lda = lda.transform(X_tf)\ny_clf = train_df['target']\n# X_train, X_test, y_train, y_test = train_test_split(X_lda, y_clf, test_size=0.2, random_state=0, stratify=y_clf)\nX_train, X_test, y_train, y_test = train_test_split(X_tf, y_clf, test_size=0.2, random_state=0, stratify=y_clf)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "341245e9729ea75108eb69fc8babd9f400835104"
      },
      "cell_type": "code",
      "source": "# y_svm = train_df['target']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f1b830134a318c3ce2a29d5d403bc97b8dd8c7a8"
      },
      "cell_type": "code",
      "source": "# from sklearn.preprocessing import MinMaxScaler\n# scaler = MinMaxScaler().fit(X_train)\n# X_train = scaler.transform(X_train)\n# X_test = scaler.transform(X_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9ec78ff20cf404d9612d7a09d9d73e4034bd70d4"
      },
      "cell_type": "code",
      "source": "# from sklearn.svm import SVC\n# clf = SVC(C=10, kernel='linear', random_state=9)\nclf = RandomForestClassifier(n_estimators=100, random_state=9)\n# clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n\nclf.fit(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0e57a7451ed97ef5f6c6d402dcc27c6375db4dfe"
      },
      "cell_type": "code",
      "source": "p_pred_y = clf.predict(X_test)\nconfusion_matrix(y_test, p_pred_y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "33f2ffde33db03d57d4635bc1d83f5263228a086"
      },
      "cell_type": "code",
      "source": "tn, fp, fn, tp = confusion_matrix(y_test, p_pred_y).ravel()\n\nacc = (tp + tn) / (tn + fp + fn + tp)\nprec = tp / (tp + fp)\nrec = tp / (tp + fn)\nf1 = 2 * prec * rec / (prec + rec)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2106f7879d4e4b4d42a71b833369c01ca4c7cb7b"
      },
      "cell_type": "code",
      "source": "print(\"accuracy\", acc)\nprint(\"precision\", prec)\nprint(\"recall\", rec)\nprint(\"f1\", f1)\nprint(tn, fp, fn, tp)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f1c1c539e81445e09c9f0c3748de5e6ec7c6fe4"
      },
      "cell_type": "code",
      "source": "# X_init_lda_test = lda.transform(vectorizer.transform(test_df['p_txtcn']))\n# X_init_lda_test = scaler.transform(X_init_lda_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0ca47320fd4d8188902275695df03c8e20c95b64"
      },
      "cell_type": "code",
      "source": "# y_pred_final = clf.predict(X_init_lda_test)\ny_pred_final = clf.predict(vectorizer.transform(test_df['p_txtcn']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "38788e774292beabc8d1c468e65caf20c9459910"
      },
      "cell_type": "code",
      "source": "sub = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nsub['prediction'] = y_pred_final\nsub.to_csv(\"submission.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "025bb9348c0c3561c08a237b4176e7502b5ce28a"
      },
      "cell_type": "code",
      "source": "# X_lda[:30]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ea1105b941266e84642382f22c942f6a695b2f4"
      },
      "cell_type": "code",
      "source": "# X_lda_df = pd.DataFrame(X_lda)\n# X_lda_df[\"idxmax\"] = X_lda_df.idxmax(axis=1)\n# X_lda_df.head(2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b2f52f01b288533a0be1b1769a361ccd81803c3e"
      },
      "cell_type": "code",
      "source": "# y_true = train_df[\"target\"]\n# y_lda = X_lda_df[\"idxmax\"]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "29ab04b1f841d93d9fd38fc68832a14e1c460e92"
      },
      "cell_type": "code",
      "source": "# print(confusion_matrix(y_true, y_lda)[:2])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d154e0cce3e2571ac647779024a42f78b7ff55bd"
      },
      "cell_type": "markdown",
      "source": "#### Trying Deep Learning with GloVe"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f02899931638b7e0defb426c045a94f36e6ec5aa"
      },
      "cell_type": "code",
      "source": "def fit_get_sequences(df, vocab_size):\n    tokenizer = Tokenizer(num_words=vocab_size)\n    tokenizer.fit_on_texts(df['p_txt'])\n    sequences = tokenizer.texts_to_sequences(df['p_txt'])\n    return sequences, tokenizer, vocab_size",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9bb49ea62415ada2045514c07241e388a8489949"
      },
      "cell_type": "code",
      "source": "def get_sequences(df, tokenizer):\n    return tokenizer.texts_to_sequences(df['p_txt'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "794589d6f3e18fb4820355909e5eec559cd9d2e7"
      },
      "cell_type": "code",
      "source": "# vocab_size = 1000\n# sequences, tokenizer, vocab_size = fit_get_sequences(train_df, vocab_size)\n# word_index = tokenizer.word_index\n# print('Found %s unique tokens.' % len(word_index))\n# avg = sum(map(len, sequences)) / len(sequences)\n# std = np.sqrt(sum(map(lambda x: (len(x) - avg) ** 2, sequences)) / len(sequences))\n# print(\"Tokens avg {} and std {}\".format(avg, std))\n\n# max_length = 100\n# X = pad_sequences(sequences, maxlen=max_length)\n# X_init_test = pad_sequences(get_sequences(test_df, tokenizer), maxlen=max_length)\n\n# y = to_categorical(np.asarray(train_df['target']))\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c7b21b54e9a25702b44650424b0e9f71151b20d"
      },
      "cell_type": "code",
      "source": "# y = to_categorical(np.asarray(train_df['target']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d48f305db9ad82cb59148ca481aac02a53d03994"
      },
      "cell_type": "code",
      "source": "# print('Shape of data:', X.shape)\n# print('Shape of labels:', y.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1868fd3037449e63752c14b183453c678ba4ecf0"
      },
      "cell_type": "code",
      "source": "def generate_embeddings(vocab_size, word_index):\n    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n    embeddings_index = {}\n    f = open(EMBEDDING_FILE)\n    # In the dataset, each line represents a new word embedding\n    # The line starts with the word and the embedding values follow\n#     for line in f:\n#         values = line.split()\n#         word = values[0]\n#         embedding = np.asarray(values[1:], dtype='float32')\n#         embeddings_index[word] = embedding \n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in f if o.split(\" \")[0] in word_index)\n    f.close()\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean = all_embs.mean()\n    emb_std = all_embs.std()\n    print(emb_mean, emb_std)\n    embedding_dim = 300\n    nb_words = min(vocab_size, len(word_index))  # How many words are there actually\n    # Create a random matrix with the same mean and std as the embeddings\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_dim))\n    # The vectors need to be in the same position as their index.\n    # Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n    # Loop over all words in the word index\n    for word, i in word_index.items():\n        # If we are above the amount of words we want to use we do nothing\n        if i >= vocab_size:\n            break\n        # Get the embedding vector for the word\n        embedding_vector = embeddings_index.get(word)\n        # If there is an embedding vector, put it in the embedding matrix\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    return embedding_dim, embedding_matrix    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "56800892d2ddd58a63f06127c9c49a9447369deb"
      },
      "cell_type": "code",
      "source": "# embedding_dim, embedding_matrix = generate_embeddings(vocab_size, word_index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cc076eee75a3cbd2c40d4a65e23a1ddcd7f034f3"
      },
      "cell_type": "code",
      "source": "def generate_model(embedding_dim, embedding_matrix, max_length, vocab_size, ifembed=True):\n    model = Sequential()\n    # model.add(Embedding(vocab_size, 100, input_length=max_length))\n    if ifembed:\n        model.add(\n            Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embedding_matrix], trainable=False))\n        model.add(LSTM(32, return_sequences=True))\n    else:\n        model.add(LSTM(32, return_sequences=True, input_shape=(max_length)))\n    model.add(Dropout(0.2))\n    model.add(LSTM(32, return_sequences=False))\n    model.add(Dropout(0.2))\n    model.add(Dense(2))\n    model.add(Activation('softmax'))\n    model.summary()\n    model.compile(loss='binary_crossentropy', optimizer='adam',\n                  metrics=[metrics.mae, metrics.categorical_accuracy])\n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "905ccc24c9660d6790e7ce28ee42e5355fb06b10"
      },
      "cell_type": "code",
      "source": "# model = generate_model(embedding_dim, embedding_matrix, max_length, vocab_size)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "135104fa288d21fb9c6e1665bddb102308227627"
      },
      "cell_type": "code",
      "source": "# batch_size = 1000\n# model.fit(X_train, y_train, batch_size=batch_size, epochs=10, validation_split=0.1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ca8b6a659113073b37a789ae4ccf5c87eada9541"
      },
      "cell_type": "code",
      "source": "# y_svm = train_df['target']\n# X_train, X_test, y_train, y_test = train_test_split(X, y_svm, test_size=0.2, random_state=0, stratify=y)\n\n# from sklearn.svm import SVC\n# clf = SVC(C=100, kernel='linear', random_state=9)\n# clf.fit(X_train, y_train)\n# p_pred_y = clf.predict(X_test)\n\n# confusion_matrix(y_test, p_pred_y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "42ff8dd57cedc4b1227e4f92a367c4085430961f"
      },
      "cell_type": "code",
      "source": "def c_matrix(y_true, y_pred, num_classes=2):\n    cm = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n    print(cm)\n    cm_np = np.asarray(cm)\n    TP = np.diag(cm_np)\n    FP = np.sum(cm, axis=0) - TP\n    FN = np.sum(cm, axis=1) - TP\n    TN = []\n    for i in range(num_classes):\n        temp = np.delete(cm, i, 0)\n        temp = np.delete(temp, i, 1)\n        TN.append(sum(sum(temp)))\n    prec = TP / (TP + FP)\n    rec = TP / (TP + FN)\n    acc = (TP + TN) / (TP + FP + TN + FN)\n    f1 = 2 * prec * rec / (prec + rec)\n    print(\"precision\", prec)\n    print(\"recall\", rec)\n    print(\"accuracy\", acc)\n    print(\"f1\", f1)\n    return prec, rec, acc, f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3777868003dd11608e541bce9436b5da15b21d0b"
      },
      "cell_type": "code",
      "source": "# y_pred = model.predict(X_test)\n\n# c_matrix(y_test, y_pred, num_classes=2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7d8b6150b53443b71ae5caba24980fa5e6cbe412"
      },
      "cell_type": "code",
      "source": "# y_pred_final = model.predict(X_init_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "53ebd4ab8be7625789c8d6315a01ddd9cb6c09b3"
      },
      "cell_type": "code",
      "source": "# y_1 = y_pred_final.argmax(axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a943cb49d6b8ed68591ca6e94503a5e6163bc16b"
      },
      "cell_type": "code",
      "source": "# sub = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n# sub['prediction'] = y_1\n# sub.to_csv(\"submission.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb9120991ee58fedae001028ccf22680380f12aa"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}