{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "56b80176f1af4f1b90a1acfd7b0ecf11ab782d4c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport string, re\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.metrics import confusion_matrix\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import wordpunct_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport nltk",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8e16fc01dc31e4d838a86c538cae65bf2554d68f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Activation, Embedding, Dropout, Bidirectional, CuDNNLSTM\nfrom keras import metrics\nfrom keras.utils import to_categorical",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c54a1a3e0fb6a29c80cd644b5c685b9be26ac252",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.svm import SVC",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6835db2d355393ba6124fcf8a7cf8c3893e75a88",
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_raw_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\ntrain_df = train_raw_df\ntrain_df = train_raw_df.sample(frac=0.5)\ntrain_y_sample = train_raw_df[train_raw_df['target'] == 1].sample(frac=0.9)\ntrain_df = train_df.append(train_y_sample)\ntrain_n = len(train_df)\ntest_n = len(test_df)\nprint(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6e9ad7e57b7e9a9ef61355f78e2991402b4502b4",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# train_df.groupby('target').count()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "617609719444fe6c761d022e10fb6d8fcde9a400",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# train_df.head(1)\n# test_df[test_df[\"qid\"] == \"016eaf1d76c7a8a288b0\"].iloc[0][\"question_text\"]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "43c12d450ed66e081b7aa5e486a656482e80914c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "lemmatizer = WordNetLemmatizer()\nstopset = set(stopwords.words('english'))\nstopset_min = {'the'}\n# stopset_min = stopset",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2f3db298d15bd0cd6eb3ed6d7d5b488e016ad432",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def process(txt, use_small=True):\n    tokens = []\n    for token in wordpunct_tokenize(txt):\n        if len(token) <= 2:\n            continue\n        if token.isdigit():\n            continue\n        if all(char in string.punctuation for char in token):\n            continue\n        \n        token = token.lower()\n        token = token.strip()  # Strip whitespace and other punctuations\n        token = token.strip('_')  # remove _ if any\n        token = token.strip('*')\n        if (use_small and token in stopset_min) or (not use_small and token in stopset):\n            continue\n        tokens.append(token)\n#         lemmatizer.lemmatize(token)\n    return tokens",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4885b08cca7bb1c44c15febbdc8d4194998cd17e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_df['p_txt'] = train_df['question_text'].apply(process)\ntest_df['p_txt'] = test_df['question_text'].apply(process)\n# train_df['p_txt_lda'] = train_df['question_text'].apply(lambda txt: process(txt, use_small=False))\n# test_df['p_txt_lda'] = test_df['question_text'].apply(lambda txt: process(txt, use_small=False))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5902258b272dc7020ef1c68b4166e3ef0fb0fe7f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# train_df['p_txtcn'] = train_df['p_txt'].apply(lambda tokens: ' '.join(str(v) for v in tokens))\n# test_df['p_txtcn'] = test_df['p_txt'].apply(lambda tokens: ' '.join(str(v) for v in tokens))\n# train_df['p_txt_lda'] = train_df['p_txt_lda'].apply(lambda tokens: ' '.join(str(v) for v in tokens))\n# test_df['p_txt_lda'] = test_df['p_txt_lda'].apply(lambda tokens: ' '.join(str(v) for v in tokens))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "52350f70211e4009a4cff2c4fea7ebc4255036f9",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# num_freq_features = 25000\n# num_remove_features = 50\n# tf_vectorizer = CountVectorizer(max_features=num_freq_features)\n# bag_of_words = tf_vectorizer.fit_transform(train_df['p_txtcn'])\n# sum_words = bag_of_words.sum(axis=0)\n# words_freq = [(word, sum_words[0, idx]) for word, idx in tf_vectorizer.vocabulary_.items()]\n# words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n# top_words = set([w[0] for w in words_freq[:min(num_remove_features, len(words_freq))]])\n# len_freq_words = len(top_words) if top_words else 0",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6e8ccf2888f352036b024526e7a8f9acaded477d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# train_df['p_txt'] = train_df['p_txt'].apply(lambda x: list(set(x) - top_words))\n# test_df['p_txt'] = test_df['p_txt'].apply(lambda x: list(set(x) - top_words))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fbba0bae21578708f62c6725ed1f0cde32d733d1",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_df.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a9cdef67d2482aec4b9787120b5ebc987c587d69",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# train_df['p_txtcn'] = train_df['p_txt'].apply(lambda tokens: ' '.join(str(v) for v in tokens))\n# test_df['p_txtcn'] = test_df['p_txt'].apply(lambda tokens: ' '.join(str(v) for v in tokens))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7a9f0f94bcfb6224460eec9bf1d980056d1eb702",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# no_features = (num_freq_features - len_freq_words)\nno_features = 50000\ntfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=no_features)\ntfidf_vectorizer_lda = TfidfVectorizer(ngram_range=(1, 3), max_features=no_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "27f60507665d70b532a04ab6f479af1b2aeccf24",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def vectorize(txt, vectorizer):\n    X = vectorizer.fit_transform(txt)\n    feature_names = vectorizer.get_feature_names()\n    return X, feature_names, vectorizer",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a575684c07286add20669c7941bac4a04d25b684",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# X_tf, tf_feature_names, vectorizer = vectorize(train_df['p_txtcn'], tfidf_vectorizer)\n# X_tf_lda, tf_feature_names_lda, vectorizer_lda = vectorize(train_df['p_txt_lda'], tfidf_vectorizer_lda)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1f7b0498071a24169514342d5e6a7a7937763269",
        "trusted": true
      },
      "cell_type": "code",
      "source": "no_topics = 25\nnum_iter = 5",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "97698464aff47dd1c030ce2c80f05a481a3ad051",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# lda = LatentDirichletAllocation(n_components=no_topics, max_iter=num_iter, learning_method='online', learning_offset=50.,random_state=9).fit(X_tf_lda)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d61ed404a47066640545b863ad56ed43c12cc65a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def display_topics(model, feature_names, no_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        top_features = []\n        for i in topic.argsort()[:-no_top_words - 1:-1]:\n            if topic[i] >  1.0:\n                top_features.append((feature_names[i], topic[i]))\n            else:\n                continue        \n        print(\"Topic %d:\" % (topic_idx))\n        print(\", \".join([str(val[0])+\": \"+\"%.2f\" % val[1] for val in top_features]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "00ed3c18da2bbdf307de9b30607320e4aeae3bc6",
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# display_topics(lda, tf_feature_names_lda, 5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f07f3955309c9f0cb37bce60f265e02109f29056"
      },
      "cell_type": "code",
      "source": "y_clf = train_df['target']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f3d2e8d1b67e1dceb4c08ae43e55eee16b325c15"
      },
      "cell_type": "code",
      "source": "# X_train, X_test, y_train, y_test = train_test_split(X_tf, y_clf, test_size=0.2, stratify=y_clf)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "76b030e9f0ce61db12ef3bbd740a5e53d135d13d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# X_lda = lda.transform(X_tf_lda)\n# X_lda_train, X_lda_test, y_lda_train, y_lda_test = train_test_split(X_lda, y_clf, test_size=0.2, stratify=y_clf)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "341245e9729ea75108eb69fc8babd9f400835104",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# y_svm = train_df['target']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f1b830134a318c3ce2a29d5d403bc97b8dd8c7a8",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# from sklearn.preprocessing import MinMaxScaler\n# scaler = MinMaxScaler().fit(X_train)\n# X_train = scaler.transform(X_train)\n# X_test = scaler.transform(X_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9ec78ff20cf404d9612d7a09d9d73e4034bd70d4",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# from sklearn.svm import SVC\n# clf = SVC(C=10, kernel='linear', random_state=9)\nclf = RandomForestClassifier(n_estimators=100, random_state=9)\n# clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n\n# clf.fit(X_train, y_train)\n\n# clf2 = RandomForestClassifier(n_estimators=100, random_state=9)\n# clf2 = SVC(C=10, kernel='linear', random_state=9)\n# clf2.fit(X_lda_train, y_lda_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0e57a7451ed97ef5f6c6d402dcc27c6375db4dfe",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# p_pred_y = clf.predict(X_test)\n# print(confusion_matrix(y_test, p_pred_y))\n\n# p_pred_y_lda = clf2.predict(X_lda_test)\n# print(confusion_matrix(y_lda_test, p_pred_y_lda))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "33f2ffde33db03d57d4635bc1d83f5263228a086",
        "trusted": false
      },
      "cell_type": "code",
      "source": "def print_cm(y_test, p_pred_y):\n    tn, fp, fn, tp = confusion_matrix(y_test, p_pred_y).ravel()\n\n    acc = (tp + tn) / (tn + fp + fn + tp)\n    prec = tp / (tp + fp)\n    rec = tp / (tp + fn)\n    f1 = 2 * prec * rec / (prec + rec)\n\n    print(\"accuracy\", acc)\n    print(\"precision\", prec)\n    print(\"recall\", rec)\n    print(\"f1\", f1)\n    print(tn, fp, fn, tp)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2106f7879d4e4b4d42a71b833369c01ca4c7cb7b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# print_cm(y_test, p_pred_y)\n# print_cm(y_lda_test, p_pred_y_lda)\n\n# print_cm(p_pred_y, p_pred_y_lda)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4f1c1c539e81445e09c9f0c3748de5e6ec7c6fe4",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# X_init_lda_test = lda.transform(vectorizer_lda.transform(test_df['p_txt_lda']))\n# X_init_lda_test = scaler.transform(X_init_lda_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0ca47320fd4d8188902275695df03c8e20c95b64",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# y_pred_final = clf.predict(vectorizer.transform(test_df['p_txtcn']))\n# y_pred_final_lda = clf2.predict(X_init_lda_test)\n\n# y_1 = np.maximum(y_pred_final, y_pred_final_lda)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "38788e774292beabc8d1c468e65caf20c9459910",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# sub = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n# sub['prediction'] = y_1\n# sub.to_csv(\"submission.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d154e0cce3e2571ac647779024a42f78b7ff55bd"
      },
      "cell_type": "markdown",
      "source": "#### Trying Deep Learning with GloVe"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7338e1364dcb17a01376f9cdc8d2d2a8e50ec48e"
      },
      "cell_type": "code",
      "source": "var_name= \"p_txt\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f02899931638b7e0defb426c045a94f36e6ec5aa",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def fit_get_sequences(df, vocab_size):\n    tokenizer = Tokenizer(num_words=vocab_size)\n    tokenizer.fit_on_texts(df[var_name])\n    sequences = tokenizer.texts_to_sequences(df[var_name])\n    return sequences, tokenizer, vocab_size",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9bb49ea62415ada2045514c07241e388a8489949",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_sequences(df, tokenizer):\n    return tokenizer.texts_to_sequences(df[var_name])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "794589d6f3e18fb4820355909e5eec559cd9d2e7",
        "trusted": true
      },
      "cell_type": "code",
      "source": "vocab_size = 50000\nsequences, tokenizer, vocab_size = fit_get_sequences(train_df, vocab_size)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\navg = sum(map(len, sequences)) / len(sequences)\nstd = np.sqrt(sum(map(lambda x: (len(x) - avg) ** 2, sequences)) / len(sequences))\nprint(\"Tokens avg {} and std {}\".format(avg, std))\n\nmax_length = 100\nX = pad_sequences(sequences, maxlen=max_length)\nX_init_test = pad_sequences(get_sequences(test_df, tokenizer), maxlen=max_length)\n\ny = to_categorical(np.asarray(train_df['target']))\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=0, stratify=y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8c7b21b54e9a25702b44650424b0e9f71151b20d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "y = to_categorical(np.asarray(train_df['target']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d48f305db9ad82cb59148ca481aac02a53d03994",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# print('Shape of data:', X.shape)\n# print('Shape of labels:', y.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1868fd3037449e63752c14b183453c678ba4ecf0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def generate_embeddings(vocab_size, word_index):\n    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n    embeddings_index = {}\n    f = open(EMBEDDING_FILE)\n    # In the dataset, each line represents a new word embedding\n    # The line starts with the word and the embedding values follow\n#     for line in f:\n#         values = line.split()\n#         word = values[0]\n#         embedding = np.asarray(values[1:], dtype='float32')\n#         embeddings_index[word] = embedding \n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in f if o.split(\" \")[0] in word_index)\n    f.close()\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean = all_embs.mean()\n    emb_std = all_embs.std()\n    print(emb_mean, emb_std)\n    embedding_dim = 300\n    nb_words = min(vocab_size, len(word_index))  # How many words are there actually\n    # Create a random matrix with the same mean and std as the embeddings\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_dim))\n    # The vectors need to be in the same position as their index.\n    # Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n    # Loop over all words in the word index\n    for word, i in word_index.items():\n        # If we are above the amount of words we want to use we do nothing\n        if i >= vocab_size:\n            break\n        # Get the embedding vector for the word\n        embedding_vector = embeddings_index.get(word)\n        # If there is an embedding vector, put it in the embedding matrix\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    return embedding_dim, embedding_matrix    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "56800892d2ddd58a63f06127c9c49a9447369deb",
        "trusted": true
      },
      "cell_type": "code",
      "source": "embedding_dim, embedding_matrix = generate_embeddings(vocab_size, word_index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cc076eee75a3cbd2c40d4a65e23a1ddcd7f034f3",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def generate_model(embedding_dim, embedding_matrix, max_length, vocab_size, ifembed=True):\n    model = Sequential()\n    # model.add(Embedding(vocab_size, 100, input_length=max_length))\n    if ifembed:\n        model.add(\n            Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embedding_matrix], trainable=False))\n        model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\n    else:\n        model.add(LSTM(32, return_sequences=True, input_shape=(max_length)))\n    model.add(Dropout(0.2))\n    model.add(Bidirectional(CuDNNLSTM(64, return_sequences=True)))\n    model.add(Dropout(0.2))\n    model.add(Bidirectional(CuDNNLSTM(64, return_sequences=True)))\n    model.add(Dropout(0.2))\n    model.add(Bidirectional(CuDNNLSTM(64, return_sequences=False)))\n    model.add(Dropout(0.2))\n    model.add(Dense(2))\n    model.add(Activation('softmax'))\n    model.summary()\n    model.compile(loss='binary_crossentropy', optimizer='adam',\n                  metrics=[metrics.mae, metrics.categorical_accuracy])\n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "905ccc24c9660d6790e7ce28ee42e5355fb06b10",
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = generate_model(embedding_dim, embedding_matrix, max_length, vocab_size)",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'generate_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6bf97642d529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_model' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "135104fa288d21fb9c6e1665bddb102308227627",
        "trusted": true
      },
      "cell_type": "code",
      "source": "batch_size = 5000\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=20, validation_split=0.1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ca8b6a659113073b37a789ae4ccf5c87eada9541",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# y_svm = train_df['target']\n# X_train, X_test, y_train, y_test = train_test_split(X, y_svm, test_size=0.2, random_state=0, stratify=y)\n\n# from sklearn.svm import SVC\n# clf = SVC(C=100, kernel='linear', random_state=9)\n# clf.fit(X_train, y_train)\n# p_pred_y = clf.predict(X_test)\n\n# confusion_matrix(y_test, p_pred_y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "42ff8dd57cedc4b1227e4f92a367c4085430961f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def c_matrix(y_true, y_pred, num_classes=2):\n    cm = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n    print(cm)\n    cm_np = np.asarray(cm)\n    TP = np.diag(cm_np)\n    FP = np.sum(cm, axis=0) - TP\n    FN = np.sum(cm, axis=1) - TP\n    TN = []\n    for i in range(num_classes):\n        temp = np.delete(cm, i, 0)\n        temp = np.delete(temp, i, 1)\n        TN.append(sum(sum(temp)))\n    prec = TP / (TP + FP)\n    rec = TP / (TP + FN)\n    acc = (TP + TN) / (TP + FP + TN + FN)\n    f1 = 2 * prec * rec / (prec + rec)\n    print(\"precision\", prec)\n    print(\"recall\", rec)\n    print(\"accuracy\", acc)\n    print(\"f1\", f1)\n    return prec, rec, acc, f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3777868003dd11608e541bce9436b5da15b21d0b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# y_pred = model.predict(X_test)\n\n# c_matrix(y_test, y_pred)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7d8b6150b53443b71ae5caba24980fa5e6cbe412",
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_pred_final_dl = model.predict(X_init_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "53ebd4ab8be7625789c8d6315a01ddd9cb6c09b3",
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_1 = y_pred_final_dl.argmax(axis=1)\ny_2 = y_1\n# y_2 = np.maximum(y_pred_final_lda, y_1)\n# y_2 = np.maximum(y_pred_final, y_1)\n# y_2 = np.maximum(y_pred_final, y_pred_final_lda, y_1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a943cb49d6b8ed68591ca6e94503a5e6163bc16b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "sub = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nsub['prediction'] = y_2\nsub.to_csv(\"submission.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bb9120991ee58fedae001028ccf22680380f12aa",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}