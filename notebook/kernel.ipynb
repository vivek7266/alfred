{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "56b80176f1af4f1b90a1acfd7b0ecf11ab782d4c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string, re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "8e16fc01dc31e4d838a86c538cae65bf2554d68f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, Embedding, Dropout\n",
    "from keras import metrics\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "c54a1a3e0fb6a29c80cd644b5c685b9be26ac252"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "6835db2d355393ba6124fcf8a7cf8c3893e75a88"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (211422, 3)\n",
      "Test shape :  (56370, 2)\n"
     ]
    }
   ],
   "source": [
    "train_raw_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "train_df = train_raw_df\n",
    "train_df = train_raw_df.sample(frac=0.1)\n",
    "train_df = train_df.append(train_raw_df[train_raw_df['target'] == 1])\n",
    "train_n = len(train_df)\n",
    "test_n = len(test_df)\n",
    "print(\"Train shape : \",train_df.shape)\n",
    "print(\"Test shape : \",test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "6e9ad7e57b7e9a9ef61355f78e2991402b4502b4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122513</td>\n",
       "      <td>122513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88909</td>\n",
       "      <td>88909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           qid  question_text\n",
       "target                       \n",
       "0       122513         122513\n",
       "1        88909          88909"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('target').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "617609719444fe6c761d022e10fb6d8fcde9a400"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Has anyone gotten banned on Quora for being under 13?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(1)\n",
    "test_df.iloc[594,:][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "43c12d450ed66e081b7aa5e486a656482e80914c"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stopset = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "2f3db298d15bd0cd6eb3ed6d7d5b488e016ad432"
   },
   "outputs": [],
   "source": [
    "def process(txt):\n",
    "    tokens = []\n",
    "    for token in wordpunct_tokenize(txt):\n",
    "        if token.isdigit():\n",
    "            continue\n",
    "        if all(char in string.punctuation for char in token):\n",
    "            continue\n",
    "        \n",
    "        token = token.lower()\n",
    "        token = token.strip()  # Strip whitespace and other punctuations\n",
    "        token = token.strip('_')  # remove _ if any\n",
    "        token = token.strip('*')\n",
    "        if token in stopset:\n",
    "            continue\n",
    "        tokens.append(token)\n",
    "        lemmatizer.lemmatize(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "4885b08cca7bb1c44c15febbdc8d4194998cd17e"
   },
   "outputs": [],
   "source": [
    "train_df['p_txt'] = train_df['question_text'].apply(process)\n",
    "test_df['p_txt'] = test_df['question_text'].apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "fbba0bae21578708f62c6725ed1f0cde32d733d1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>p_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1302852</th>\n",
       "      <td>ff5ce163099aa6075efc</td>\n",
       "      <td>What are some examples of well-written economi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[examples, well, written, economic, papers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898515</th>\n",
       "      <td>b00b8adb04ab754554ca</td>\n",
       "      <td>What are some tips for starting a new school y...</td>\n",
       "      <td>0</td>\n",
       "      <td>[tips, starting, new, school, junior, year]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182181</th>\n",
       "      <td>e7aa94d5d64f6cb52199</td>\n",
       "      <td>What is NBT (National Book Trust)?</td>\n",
       "      <td>0</td>\n",
       "      <td>[nbt, national, book, trust]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627266</th>\n",
       "      <td>7ad84f5c5aa7416f9a27</td>\n",
       "      <td>Are there any existing photos of Michio Kaku's...</td>\n",
       "      <td>0</td>\n",
       "      <td>[existing, photos, michio, kaku, betatron, ope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697138</th>\n",
       "      <td>88852a7e529aba265889</td>\n",
       "      <td>How do we evaluate people?</td>\n",
       "      <td>0</td>\n",
       "      <td>[evaluate, people]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          qid                                      question_text  target                                              p_txt\n",
       "1302852  ff5ce163099aa6075efc  What are some examples of well-written economi...       0        [examples, well, written, economic, papers]\n",
       "898515   b00b8adb04ab754554ca  What are some tips for starting a new school y...       0        [tips, starting, new, school, junior, year]\n",
       "1182181  e7aa94d5d64f6cb52199                 What is NBT (National Book Trust)?       0                       [nbt, national, book, trust]\n",
       "627266   7ad84f5c5aa7416f9a27  Are there any existing photos of Michio Kaku's...       0  [existing, photos, michio, kaku, betatron, ope...\n",
       "697138   88852a7e529aba265889                         How do we evaluate people?       0                                 [evaluate, people]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "35bdd3ef78726d0b42fb827439810578f9da4edf"
   },
   "outputs": [],
   "source": [
    "train_df['p_txtcn'] = train_df['p_txt'].apply(lambda tokens: ' '.join(str(v) for v in tokens))\n",
    "test_df['p_txtcn'] = test_df['p_txt'].apply(lambda tokens: ' '.join(str(v) for v in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "7a9f0f94bcfb6224460eec9bf1d980056d1eb702"
   },
   "outputs": [],
   "source": [
    "no_features = 500\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=no_features)\n",
    "# tf_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "27f60507665d70b532a04ab6f479af1b2aeccf24"
   },
   "outputs": [],
   "source": [
    "def vectorize(txt, vectorizer):\n",
    "    X = vectorizer.fit_transform(txt)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    return X, feature_names, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "a575684c07286add20669c7941bac4a04d25b684"
   },
   "outputs": [],
   "source": [
    "# X_tf, tf_feature_names, vectorizer = vectorize(train_df['p_txtcn'], tfidf_vectorizer)\n",
    "X_tf_lda, tf_feature_names_lda, vectorizer_lda = vectorize(train_df['p_txtcn'], tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "1f7b0498071a24169514342d5e6a7a7937763269"
   },
   "outputs": [],
   "source": [
    "no_topics = 10\n",
    "num_iter = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "97698464aff47dd1c030ce2c80f05a481a3ad051"
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=num_iter, learning_method='online', learning_offset=50.,random_state=9, evaluate_every=-1).fit(X_tf_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "d61ed404a47066640545b863ad56ed43c12cc65a"
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features = []\n",
    "        for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
    "            if topic[i] >  1.0:\n",
    "                top_features.append((feature_names[i], topic[i]))\n",
    "            else:\n",
    "                continue        \n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\", \".join([str(val[0])+\": \"+\"%.2f\" % val[1] for val in top_features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "00ed3c18da2bbdf307de9b30607320e4aeae3bc6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "men: 3233.52, sex: 2905.78, liberals: 2379.40, make: 1965.18, support: 972.60\n",
      "Topic 1:\n",
      "americans: 2989.64, girls: 2856.86, get: 2680.32, gay: 1853.83, true: 1697.88\n",
      "Topic 2:\n",
      "think: 3567.83, black: 2795.41, trump: 2187.86, american: 1901.79, people: 1896.63\n",
      "Topic 3:\n",
      "india: 3162.79, chinese: 2299.92, much: 2060.40, feel: 1699.90, good: 1664.96\n",
      "Topic 4:\n",
      "women: 5626.34, would: 3016.58, us: 2587.84, jews: 1772.29, believe: 1558.40\n",
      "Topic 5:\n",
      "want: 2278.29, america: 1683.73, democrats: 1486.76, china: 1368.38, countries: 1327.58\n",
      "Topic 6:\n",
      "trump: 3358.30, white: 3279.00, indian: 3178.71, people: 1552.02, racist: 1407.94\n",
      "Topic 7:\n",
      "muslims: 3524.84, hate: 2679.74, muslim: 2056.69, one: 1771.60, world: 1481.78\n",
      "Topic 8:\n",
      "people: 4184.74, many: 3306.77, indians: 2878.93, country: 1908.54, really: 1613.29\n",
      "Topic 9:\n",
      "like: 3997.61, quora: 3592.62, president: 1569.47, questions: 1402.33, even: 1187.51\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda, tf_feature_names_lda, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1837258728076321289394716\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1837258728076321289394716_data = {\"mdsDat\": {\"x\": [-117.72761535644531, 113.92729187011719, 253.28970336914062, -258.65045166015625, -375.5523681640625, -5.128245830535889, 499.7063903808594, 4.754778861999512, -501.4119873046875, 375.205810546875], \"y\": [-132.59422302246094, 95.09032440185547, -278.33416748046875, 242.4319305419922, -389.4633483886719, -520.2916259765625, -24.195478439331055, 480.6818542480469, -16.82289695739746, 348.3943786621094], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [10.938934480228657, 10.268970308767784, 10.175497140066803, 10.149836566282886, 9.960029348748487, 9.931057567846164, 9.875111667003242, 9.743104166862121, 9.505497099001706, 9.451961655192163]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"Freq\": [4493.0, 4442.0, 3269.0, 3153.0, 4411.0, 3033.0, 7624.0, 3270.0, 2673.0, 2689.0, 2702.0, 2606.0, 2485.0, 2400.0, 2446.0, 2338.0, 2327.0, 2206.0, 2806.0, 2610.0, 2117.0, 2091.0, 1965.0, 1976.0, 2067.0, 1908.0, 1623.0, 1643.0, 1630.0, 1597.0, 4492.7338707661675, 2066.4361084035977, 1415.2088036565963, 1244.414598826918, 1213.5112254980684, 1147.3374901389402, 1120.0135024420197, 1093.1175890793368, 922.7413764165854, 838.8160840064728, 788.3949625886113, 749.4759674205623, 668.1528112760678, 645.692360736999, 579.9663281721564, 574.0344567737304, 473.84300752677717, 472.1978401485754, 455.0100292139098, 439.45321435956635, 438.07447891381725, 432.0184448486949, 427.88331818989536, 421.6869260768252, 416.62510054682224, 412.332232212656, 402.4527238566166, 395.3953723029144, 375.86374487087096, 363.86689482667714, 408.54549402454575, 2408.79535635925, 952.3140163384699, 428.9069386854544, 2702.1897330514507, 2117.1784063014475, 1440.3726085820947, 1238.5151582335704, 1192.9517680955005, 1161.7977919172577, 1128.7421697766756, 976.2608967021736, 957.8549901138966, 832.4055922672769, 778.2913185734269, 767.4786001926485, 758.9402690214545, 698.4256893649457, 654.0360840645144, 640.3795227251749, 581.97774339348, 571.0704967232784, 554.7219541075975, 518.2682573943443, 485.8515262035492, 476.42566560981015, 470.2417904425283, 439.51748890342253, 403.4456629015534, 396.803898773944, 393.2681817336231, 385.08335750927915, 356.2724762823667, 348.42343911215767, 1657.0334862727445, 1436.4658756061433, 599.1910268824635, 2090.515668418691, 1544.954063111414, 1364.2248460709288, 1255.5984828487592, 1218.164264707691, 1198.0117425030917, 1190.8778887791145, 1116.5766907170112, 1090.269497998289, 1072.5987631688529, 1064.5357175005292, 1040.8048488022087, 975.353703507415, 917.7838285615521, 882.5257995787308, 813.9812685657964, 741.8996370910935, 682.3022834005542, 1056.1957350963346, 629.6927879423793, 564.8713869811986, 558.4325051434647, 552.5550772076065, 468.5227608361716, 462.5382836908674, 412.2346338082655, 366.35633755333436, 355.76443521411744, 350.43773745227816, 342.16248794299594, 660.5401807564053, 934.4523699820677, 903.6722503509836, 421.9067484217535, 2688.3650141297635, 2606.137519298742, 1154.3329794864899, 957.3248022966757, 858.7348238945922, 828.4138946587577, 778.155238350735, 742.5216446367434, 737.9492673585077, 706.2474639206874, 698.7148947438991, 652.3935134555605, 652.3574879150442, 607.7027070745778, 606.2610567791669, 600.8503943910309, 596.6856704895443, 588.5211312185857, 582.3852012511007, 573.1798154190196, 566.0626249909644, 539.4525965714844, 523.0239283685731, 510.7027005024437, 464.31637564965786, 448.70344342810097, 433.9781596338853, 427.25365100349455, 406.2506193803985, 382.1831854696294, 2753.3804115212192, 1272.4567421737233, 2672.446505674556, 2326.67609403052, 1542.4331322140977, 1303.8168291517538, 994.8678158455486, 936.5660698610819, 921.4137926498719, 905.7730153425523, 789.3311329011808, 727.1126930992546, 712.4322334424655, 657.2205894759785, 655.5815804603255, 608.901715935288, 597.7898742899094, 582.7759036386617, 567.7679511055604, 559.213443480879, 554.9719131200678, 553.2261290460289, 542.1338928838044, 540.5693237057487, 533.6593480587358, 527.3585351392901, 526.0153509597113, 525.1898831127127, 517.9786816288523, 504.49609720862975, 462.88516476383666, 456.1820745407833, 820.8775732133646, 3382.002932820072, 2445.975909507507, 2337.3470668555847, 1516.712335494937, 1389.1237774525823, 1143.1872575206298, 1085.4698243435053, 959.8687645954732, 956.688305872125, 870.2828754542112, 767.1792982106125, 700.3412862021046, 686.3187962006904, 681.239282177222, 679.0785337374373, 640.1688564169011, 638.4998968034986, 636.8521797307474, 632.4572278759921, 525.5125677257278, 509.7169994893582, 502.384508364022, 492.3276355483447, 479.90972384612934, 476.64443307749286, 461.1301281187706, 412.9678693034044, 404.87009109622744, 403.4859828161036, 399.34543712433094, 391.6848056122311, 2192.9049632705464, 3032.6290428526495, 2205.271395785979, 1975.6091309799824, 1629.9412011538209, 1596.4361347981817, 1346.5396262952884, 1171.505326332524, 1102.589067196747, 1063.2174680920007, 919.9670345106977, 826.0753290228305, 783.8664501216793, 760.4358398680082, 735.2262437497279, 685.109558354697, 589.7686133031859, 495.15470635871924, 490.0878987461095, 486.7793879069362, 462.23470748682024, 453.67773872986874, 429.0896539372475, 416.6953092762871, 414.956170545208, 373.10030005950966, 369.5065668891177, 367.6592126342082, 367.0771376672652, 365.3590419265522, 357.77394697793534, 948.7293427036383, 799.5537322346637, 3268.7875673246167, 2485.0760736269704, 1907.2887775129186, 1642.9074262796153, 1156.6994195391205, 1116.7893856106666, 1068.94289259551, 819.2414433201106, 793.8085748444666, 718.0869021041952, 680.9580985464584, 640.4376547570063, 637.0392555637866, 623.1844005947662, 579.9390635560617, 553.2081779327323, 526.5744440634157, 519.4027702339939, 511.0115948341598, 510.6941422257396, 504.21149505517496, 455.94685965787806, 455.19491622036116, 440.4358191726515, 437.3149744107927, 425.21388676826984, 402.46305554816485, 390.76662965920457, 364.4768678341212, 362.1122034361841, 1374.1379141441985, 702.2864611266682, 417.2598257364194, 3152.3947859310438, 1377.1502095147666, 1230.4910079182791, 1031.0691391202433, 888.8388662522822, 801.8559687499389, 748.8841219129685, 741.5591234330097, 708.9888116614269, 622.3668788872523, 618.850214254744, 610.3568188843485, 581.8851990837895, 563.0390142022379, 530.4741244520214, 522.2686456792025, 519.4715529086016, 517.0888446848329, 513.7280984765148, 505.5252094350673, 494.2723033449548, 473.9011913509031, 472.9161015847454, 464.94849566831476, 461.4561517211338, 449.5009555700442, 447.55551016885045, 435.668570296876, 413.5171386557695, 391.5873919102467, 3507.7580381796383, 1041.9962756871255, 2399.8358636733233, 1965.1071179734129, 1623.0092009328405, 803.2548719992558, 716.0928216072988, 687.7177122071848, 665.3789362867179, 655.9440933660482, 649.7079550095164, 615.6205038643953, 607.0598490256364, 598.3339925624738, 559.6340634503488, 555.3181196026384, 553.4635576529481, 546.4467233161851, 535.4191274877321, 518.521139303158, 517.7174797925363, 488.35469341459697, 479.7832995490724, 465.80607771350094, 465.38797063079204, 455.090213550398, 439.07666656424703, 433.59920266412104, 416.1939922958212, 411.9112012760111, 411.12289423911795, 393.57722507933863, 2670.512906832635, 629.2352494945113], \"Term\": [\"women\", \"like\", \"muslims\", \"quora\", \"trump\", \"india\", \"people\", \"men\", \"many\", \"white\", \"think\", \"indian\", \"hate\", \"sex\", \"americans\", \"girls\", \"indians\", \"chinese\", \"would\", \"get\", \"black\", \"want\", \"liberals\", \"much\", \"us\", \"muslim\", \"make\", \"one\", \"feel\", \"good\", \"women\", \"us\", \"jews\", \"believe\", \"say\", \"god\", \"woman\", \"man\", \"government\", \"usa\", \"find\", \"left\", \"instead\", \"call\", \"child\", \"made\", \"something\", \"reason\", \"today\", \"european\", \"parents\", \"daughter\", \"let\", \"terrorist\", \"normal\", \"citizens\", \"guns\", \"respect\", \"die\", \"mental\", \"happen\", \"would\", \"ever\", \"possible\", \"think\", \"black\", \"american\", \"know\", \"donald\", \"obama\", \"donald trump\", \"take\", \"stupid\", \"wrong\", \"person\", \"every\", \"non\", \"money\", \"black people\", \"never\", \"asian\", \"fact\", \"towards\", \"hitler\", \"society\", \"show\", \"allowed\", \"lot\", \"asians\", \"wear\", \"act\", \"jesus\", \"gender\", \"election\", \"trump\", \"people\", \"men\", \"want\", \"america\", \"democrats\", \"china\", \"countries\", \"become\", \"christians\", \"hindus\", \"go\", \"kill\", \"love\", \"way\", \"look\", \"atheists\", \"religion\", \"anyone\", \"earth\", \"help\", \"best\", \"europe\", \"etc\", \"religious\", \"okay\", \"away\", \"police\", \"ok\", \"kind\", \"wants\", \"common\", \"win\", \"without\", \"like\", \"people\", \"world\", \"white\", \"indian\", \"racist\", \"pakistan\", \"come\", \"life\", \"rape\", \"real\", \"human\", \"british\", \"white people\", \"big\", \"supporters\", \"boys\", \"considered\", \"care\", \"blacks\", \"mother\", \"also\", \"bjp\", \"japanese\", \"start\", \"getting\", \"fuck\", \"yet\", \"makes\", \"well\", \"used\", \"living\", \"students\", \"trump\", \"people\", \"many\", \"indians\", \"country\", \"really\", \"old\", \"bad\", \"could\", \"girl\", \"years\", \"south\", \"conservatives\", \"mean\", \"called\", \"question\", \"ask\", \"african\", \"understand\", \"year old\", \"russian\", \"dog\", \"tell\", \"europeans\", \"uk\", \"less\", \"iq\", \"culture\", \"accept\", \"thing\", \"kids\", \"answer\", \"year\", \"people\", \"americans\", \"girls\", \"gay\", \"true\", \"right\", \"modi\", \"children\", \"stop\", \"guys\", \"hillary\", \"gun\", \"liberal\", \"media\", \"state\", \"russia\", \"sexual\", \"live\", \"clinton\", \"fake\", \"immigrants\", \"majority\", \"illegal\", \"political\", \"killed\", \"killing\", \"news\", \"social\", \"compared\", \"vote\", \"full\", \"get\", \"india\", \"chinese\", \"much\", \"feel\", \"good\", \"still\", \"islam\", \"better\", \"since\", \"north\", \"realize\", \"republicans\", \"keep\", \"first\", \"penis\", \"name\", \"population\", \"wife\", \"english\", \"average\", \"got\", \"face\", \"looking\", \"korea\", \"able\", \"taking\", \"modern\", \"likely\", \"last\", \"rich\", \"time\", \"someone\", \"muslims\", \"hate\", \"muslim\", \"one\", \"always\", \"israel\", \"use\", \"new\", \"going\", \"jewish\", \"work\", \"hindu\", \"christian\", \"school\", \"rights\", \"law\", \"another\", \"others\", \"pakistani\", \"part\", \"west\", \"two\", \"self\", \"east\", \"fat\", \"military\", \"body\", \"middle\", \"house\", \"small\", \"world\", \"give\", \"get\", \"quora\", \"president\", \"questions\", \"see\", \"anti\", \"war\", \"need\", \"states\", \"seem\", \"party\", \"western\", \"things\", \"united\", \"history\", \"control\", \"put\", \"trying\", \"guy\", \"anything\", \"terrorists\", \"united states\", \"hard\", \"change\", \"different\", \"answers\", \"great\", \"president trump\", \"allow\", \"especially\", \"nation\", \"like\", \"even\", \"sex\", \"liberals\", \"make\", \"support\", \"eat\", \"sister\", \"back\", \"everyone\", \"mom\", \"race\", \"actually\", \"male\", \"poor\", \"feminists\", \"female\", \"day\", \"son\", \"claim\", \"long\", \"around\", \"try\", \"marry\", \"family\", \"high\", \"public\", \"done\", \"using\", \"consider\", \"often\", \"everything\", \"men\", \"people\"], \"Total\": [4493.0, 4442.0, 3269.0, 3153.0, 4411.0, 3033.0, 7624.0, 3270.0, 2673.0, 2689.0, 2702.0, 2606.0, 2485.0, 2400.0, 2446.0, 2338.0, 2327.0, 2206.0, 2806.0, 2610.0, 2117.0, 2091.0, 1965.0, 1976.0, 2067.0, 1908.0, 1623.0, 1643.0, 1630.0, 1597.0, 4493.505108316579, 2067.207344204696, 1415.9799735260647, 1245.1857994818631, 1214.2824491076422, 1148.1086543895049, 1120.7846570364725, 1093.888763414129, 923.512559856653, 839.5872956211995, 789.1661212522913, 750.2471565652232, 668.923995936243, 646.4635327905071, 580.7374750084381, 574.805617800384, 474.6141964784702, 472.9690068368686, 455.78119552379076, 440.22438799974236, 438.8456312200366, 432.78960543736287, 428.6545024359877, 422.4580882275278, 417.3962522118946, 413.10341997611937, 403.2238973642288, 396.16655484008936, 376.6348765228552, 364.63802130154295, 409.54201972069853, 2806.208382097279, 1177.245733383912, 501.57247123122545, 2702.965070031387, 2117.9536792673384, 1441.1479140422343, 1239.2904634212907, 1193.7269764309547, 1162.5730650636806, 1129.5173750827826, 977.0362027648194, 958.6302676346991, 833.1808777478003, 779.0665832045423, 768.2539183842625, 759.7156012929731, 699.2009826952533, 654.8112949588927, 641.1548233280769, 582.753014045886, 571.8457975964146, 555.4972356091728, 519.0435032235389, 486.62680660418795, 477.2009346966075, 471.0170644673475, 440.2927764809322, 404.22092151352393, 397.57914303949053, 394.043446337403, 385.8586212717213, 357.0477385046549, 349.19869889499716, 4411.1072758767505, 7624.27123303674, 3270.396652314918, 2091.274967300092, 1545.713357307229, 1364.984142099903, 1256.357733354879, 1218.9235460094474, 1198.7709818577785, 1191.6371332435485, 1117.3359417029667, 1091.0287571031415, 1073.358000062435, 1065.2949748877797, 1041.5641120061478, 976.1129517387129, 918.543051235383, 883.285038927426, 814.7405324408267, 742.6588597511084, 683.0615281420148, 1057.4171402794907, 630.4520275807353, 565.6306650239134, 559.1917421545298, 553.3143334187308, 469.28202655137216, 463.2975213845832, 412.9938802973175, 367.1155638883101, 356.52370274388085, 351.1969618407383, 342.9217143020622, 837.0337559581471, 4442.8820155494095, 7624.27123303674, 2194.9526640621816, 2689.134074872812, 2606.906583312993, 1155.101999125689, 958.0938226330481, 859.5038661631156, 829.1829204291982, 778.9242632476904, 743.2906776174739, 738.7182980489824, 707.0164737829616, 699.4838623610775, 653.1625246048911, 653.1264893443167, 608.4717161135608, 607.0300651419468, 601.6194314074394, 597.4546733822064, 589.2901506515153, 583.1542617926729, 573.9488188841789, 566.8316314553804, 540.2216196415023, 523.792954537792, 511.47169953641026, 465.08542819537166, 449.47247017216125, 434.7471952861311, 428.02265463321464, 407.0196403696672, 382.9521714655905, 4411.1072758767505, 7624.27123303674, 2673.2167519304335, 2327.446287654762, 1543.2033681786204, 1304.5870442159883, 995.6379679570687, 937.3362638361984, 922.1840101871542, 906.5432122444989, 790.1013332804746, 727.8828809712496, 713.2024370786442, 657.9907742250044, 656.3517748019506, 609.6718923969011, 598.5600599593666, 583.5461037532108, 568.538146645479, 559.9835621080163, 555.7421004980623, 553.9962902697147, 542.9040821158562, 541.3395013078793, 534.4295412220524, 528.1287369775749, 526.7855059279229, 525.9600710349612, 518.7488812907548, 505.2662818792274, 463.6553496720411, 456.95225192183955, 859.3556420217047, 7624.27123303674, 2446.7451662493336, 2338.1162726057655, 1517.4815168283671, 1389.8929990053327, 1143.956498076404, 1086.2390060779303, 960.6379742482162, 957.4575212894614, 871.0520539233698, 767.9484658011421, 701.1105059582984, 687.0879911685739, 682.0084800265447, 679.8477364249628, 640.9380670262701, 639.2690976717703, 637.6213985939237, 633.2263879335962, 526.281751303889, 510.4861919299853, 503.1537458435365, 493.0968141969195, 480.6789193470742, 477.41364695696063, 461.89933100131475, 413.7370289607174, 405.6392609808382, 404.25518460657725, 400.11462779789474, 392.45398745919664, 2610.8495947894075, 3033.3841967391177, 2206.026526015859, 1976.364284859292, 1630.6963544052085, 1597.1912645384116, 1347.2947914751874, 1172.2604597126085, 1103.344196002811, 1063.9726396616375, 920.7221638973216, 826.8304756672733, 784.6215861413259, 761.190983384814, 735.9813808305946, 685.8646561965521, 590.5237187600586, 495.9098265454012, 490.84299474369817, 487.5344833185237, 462.989804650748, 454.4328558390495, 429.8447523072582, 417.45040789422586, 415.7112549571115, 373.855414943752, 370.2616806317069, 368.414326294834, 367.8322536085542, 366.114148987367, 358.52907843054874, 1158.352804448788, 976.9831224293125, 3269.54585556906, 2485.8343627715844, 1908.1485304165753, 1643.665747788204, 1157.4577179971031, 1117.5476401364435, 1069.7011664559816, 819.9997103959466, 794.5668608182425, 718.845151872169, 681.7163580698226, 641.1958972438975, 637.7975223112932, 623.9426668012228, 580.6973439477213, 553.9664453451124, 527.332732663155, 520.1610436522415, 511.76984339484346, 511.4523989080042, 504.9697447155396, 456.7051277026922, 455.9531738275388, 441.19408659749973, 438.0732117465224, 425.97214710775984, 403.2212999492805, 391.52487667638974, 365.23513036277916, 362.8704588759927, 2194.9526640621816, 912.0417643805381, 2610.8495947894075, 3153.164655762325, 1377.9134831144218, 1231.254241400155, 1031.8324250899316, 889.6021240282058, 802.619241107331, 749.6473872149626, 742.3223508202982, 709.7520723929075, 623.130132079168, 619.6134925419672, 611.120069716442, 582.6483991189801, 563.8022668715472, 531.2373921932532, 523.0319003504788, 520.234814717186, 517.8521170145517, 514.4913643900012, 506.28845892117556, 495.03549538420293, 474.66445124192893, 473.6793444678274, 465.7117432674969, 462.21936509967406, 450.26420881736277, 448.3187201075959, 436.431833485865, 414.2804184923877, 392.3506420951848, 4442.8820155494095, 1314.7442505239046, 2400.604269304891, 1965.8755713932924, 1623.7776628210017, 804.0233149738309, 716.8612044996875, 688.4860830829743, 666.1473827904285, 656.7125242078592, 650.4763219192248, 616.3889298400534, 607.8282817552699, 599.1023814035448, 560.4024912376863, 556.0865007960351, 554.2319430270242, 547.2151509242887, 536.1875249953048, 519.2895787142684, 518.4858987385252, 489.1231212761831, 480.55174340193395, 466.57448498246885, 466.1563767445931, 455.85863016709163, 439.8450676506462, 434.36763103018075, 416.9624060160018, 412.6796102424973, 411.89130290656254, 394.34564636104005, 3270.396652314918, 7624.27123303674], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 2.2127, 2.2125, 2.2123, 2.2122, 2.2122, 2.2122, 2.2122, 2.2121, 2.212, 2.2119, 2.2119, 2.2118, 2.2117, 2.2116, 2.2115, 2.2115, 2.2112, 2.2112, 2.2111, 2.2111, 2.2111, 2.2111, 2.211, 2.211, 2.211, 2.211, 2.2109, 2.2109, 2.2108, 2.2107, 2.2104, 2.0601, 2.0008, 2.0563, 2.2758, 2.2757, 2.2755, 2.2754, 2.2754, 2.2754, 2.2754, 2.2752, 2.2752, 2.2751, 2.275, 2.275, 2.275, 2.2749, 2.2749, 2.2748, 2.2747, 2.2747, 2.2746, 2.2745, 2.2744, 2.2744, 2.2744, 2.2743, 2.2741, 2.2741, 2.2741, 2.274, 2.2739, 2.2738, 1.2969, 0.6069, 0.579, 2.2848, 2.2847, 2.2846, 2.2846, 2.2846, 2.2846, 2.2846, 2.2845, 2.2845, 2.2845, 2.2845, 2.2845, 2.2844, 2.2844, 2.2843, 2.2843, 2.2842, 2.2841, 2.284, 2.284, 2.2838, 2.2838, 2.2838, 2.2836, 2.2835, 2.2833, 2.2831, 2.2831, 2.283, 2.283, 2.0484, 0.7261, 0.1526, 0.6361, 2.2874, 2.2874, 2.287, 2.2869, 2.2868, 2.2868, 2.2867, 2.2867, 2.2867, 2.2866, 2.2866, 2.2865, 2.2865, 2.2864, 2.2864, 2.2864, 2.2864, 2.2864, 2.2864, 2.2864, 2.2864, 2.2863, 2.2862, 2.2862, 2.2861, 2.286, 2.2859, 2.2859, 2.2858, 2.2857, 1.8164, 0.4973, 2.3063, 2.3063, 2.3061, 2.306, 2.3058, 2.3058, 2.3058, 2.3057, 2.3056, 2.3055, 2.3055, 2.3054, 2.3054, 2.3053, 2.3053, 2.3053, 2.3052, 2.3052, 2.3052, 2.3052, 2.3052, 2.3052, 2.3051, 2.3051, 2.3051, 2.3051, 2.3051, 2.3051, 2.3049, 2.3049, 2.2608, 1.4937, 2.3092, 2.3092, 2.309, 2.3089, 2.3088, 2.3088, 2.3087, 2.3087, 2.3086, 2.3085, 2.3084, 2.3084, 2.3084, 2.3084, 2.3083, 2.3083, 2.3083, 2.3083, 2.308, 2.308, 2.308, 2.3079, 2.3079, 2.3079, 2.3078, 2.3076, 2.3076, 2.3076, 2.3076, 2.3075, 2.1351, 2.3149, 2.3148, 2.3148, 2.3147, 2.3147, 2.3146, 2.3145, 2.3145, 2.3144, 2.3143, 2.3142, 2.3142, 2.3142, 2.3141, 2.3141, 2.3139, 2.3136, 2.3136, 2.3136, 2.3135, 2.3135, 2.3134, 2.3133, 2.3133, 2.3131, 2.3131, 2.3131, 2.3131, 2.3131, 2.313, 2.1155, 2.1147, 2.3284, 2.3283, 2.3282, 2.3281, 2.328, 2.3279, 2.3279, 2.3277, 2.3277, 2.3276, 2.3275, 2.3274, 2.3274, 2.3274, 2.3273, 2.3272, 2.3272, 2.3272, 2.3271, 2.3271, 2.3271, 2.3269, 2.3269, 2.3269, 2.3269, 2.3268, 2.3267, 2.3267, 2.3265, 2.3265, 1.8603, 2.0673, 0.4949, 2.3531, 2.3527, 2.3527, 2.3526, 2.3524, 2.3523, 2.3523, 2.3523, 2.3522, 2.3521, 2.3521, 2.3521, 2.352, 2.3519, 2.3519, 2.3518, 2.3518, 2.3518, 2.3518, 2.3518, 2.3518, 2.3517, 2.3517, 2.3517, 2.3516, 2.3516, 2.3516, 2.3515, 2.3515, 2.3514, 2.117, 2.1208, 2.3586, 2.3586, 2.3585, 2.358, 2.3579, 2.3578, 2.3578, 2.3578, 2.3578, 2.3577, 2.3577, 2.3577, 2.3576, 2.3576, 2.3576, 2.3575, 2.3575, 2.3575, 2.3575, 2.3574, 2.3573, 2.3573, 2.3573, 2.3573, 2.3572, 2.3572, 2.3571, 2.3571, 2.3571, 2.357, 2.1563, -0.1356], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.0649, -2.8416, -3.2201, -3.3487, -3.3739, -3.4299, -3.454, -3.4783, -3.6478, -3.7431, -3.8051, -3.8558, -3.9706, -4.0048, -4.1122, -4.1224, -4.3143, -4.3177, -4.3548, -4.3896, -4.3927, -4.4067, -4.4163, -4.4309, -4.443, -4.4533, -4.4776, -4.4953, -4.5459, -4.5784, -4.4625, -2.6883, -3.6162, -4.4139, -2.5101, -2.7541, -3.1393, -3.2903, -3.3278, -3.3542, -3.3831, -3.5282, -3.5472, -3.6876, -3.7548, -3.7688, -3.78, -3.8631, -3.9288, -3.9499, -4.0455, -4.0644, -4.0935, -4.1614, -4.226, -4.2456, -4.2587, -4.3263, -4.4119, -4.4285, -4.4374, -4.4585, -4.5362, -4.5585, -2.9992, -3.142, -4.0164, -2.7576, -3.06, -3.1845, -3.2674, -3.2977, -3.3144, -3.3203, -3.3848, -3.4086, -3.425, -3.4325, -3.455, -3.52, -3.5808, -3.62, -3.7009, -3.7936, -3.8773, -3.4404, -3.9576, -4.0662, -4.0777, -4.0882, -4.2532, -4.2661, -4.3812, -4.4992, -4.5285, -4.5436, -4.5675, -3.9097, -3.5628, -3.5963, -4.358, -2.5036, -2.5346, -3.349, -3.5361, -3.6448, -3.6808, -3.7433, -3.7902, -3.7964, -3.8403, -3.851, -3.9196, -3.9197, -3.9906, -3.993, -4.0019, -4.0089, -4.0227, -4.0331, -4.0491, -4.0616, -4.1097, -4.1406, -4.1645, -4.2597, -4.2939, -4.3273, -4.3429, -4.3933, -4.4544, -2.4797, -3.2516, -2.4906, -2.6292, -3.0403, -3.2083, -3.4788, -3.5392, -3.5555, -3.5726, -3.7102, -3.7923, -3.8127, -3.8934, -3.8959, -3.9697, -3.9882, -4.0136, -4.0397, -4.0549, -4.0625, -4.0656, -4.0859, -4.0888, -4.1016, -4.1135, -4.1161, -4.1176, -4.1315, -4.1578, -4.2439, -4.2585, -3.671, -2.2552, -2.5763, -2.6217, -3.0542, -3.142, -3.3369, -3.3887, -3.5117, -3.515, -3.6097, -3.7358, -3.8269, -3.8471, -3.8546, -3.8577, -3.9167, -3.9194, -3.9219, -3.9289, -4.1141, -4.1446, -4.1591, -4.1793, -4.2049, -4.2117, -4.2448, -4.3551, -4.3749, -4.3783, -4.3887, -4.408, -2.6855, -2.3556, -2.6742, -2.7842, -2.9765, -2.9973, -3.1675, -3.3068, -3.3674, -3.4038, -3.5485, -3.6561, -3.7086, -3.7389, -3.7726, -3.8432, -3.9931, -4.168, -4.1782, -4.185, -4.2368, -4.2554, -4.3112, -4.3405, -4.3447, -4.451, -4.4607, -4.4657, -4.4673, -4.4719, -4.4929, -3.5177, -3.6888, -2.2672, -2.5413, -2.8059, -2.9551, -3.306, -3.3412, -3.3849, -3.651, -3.6825, -3.7828, -3.8359, -3.8972, -3.9025, -3.9245, -3.9964, -4.0436, -4.093, -4.1067, -4.123, -4.1236, -4.1364, -4.237, -4.2386, -4.2716, -4.2787, -4.3068, -4.3618, -4.3913, -4.4609, -4.4674, -3.1338, -3.805, -4.3257, -2.2788, -3.1069, -3.2195, -3.3963, -3.5448, -3.6478, -3.7161, -3.7259, -3.7708, -3.9012, -3.9068, -3.9206, -3.9684, -4.0013, -4.0609, -4.0765, -4.0819, -4.0865, -4.093, -4.1091, -4.1316, -4.1737, -4.1758, -4.1928, -4.2003, -4.2265, -4.2309, -4.2578, -4.31, -4.3645, -2.1719, -3.3858, -2.5459, -2.7457, -2.937, -3.6404, -3.7552, -3.7957, -3.8287, -3.843, -3.8525, -3.9064, -3.9204, -3.9349, -4.0017, -4.0095, -4.0128, -4.0256, -4.046, -4.0781, -4.0796, -4.138, -4.1557, -4.1853, -4.1862, -4.2085, -4.2444, -4.2569, -4.2979, -4.3082, -4.3101, -4.3538, -2.439, -3.8845]}, \"token.table\": {\"Topic\": [7, 5, 2, 10, 5, 9, 2, 4, 8, 3, 2, 6, 8, 5, 9, 9, 3, 9, 10, 2, 2, 5, 3, 7, 3, 10, 5, 3, 1, 3, 6, 7, 4, 4, 2, 2, 4, 8, 4, 4, 1, 5, 4, 9, 1, 6, 3, 7, 8, 3, 1, 10, 6, 4, 3, 6, 5, 10, 4, 9, 5, 3, 5, 5, 1, 10, 3, 1, 9, 5, 2, 2, 10, 3, 8, 10, 2, 7, 9, 3, 3, 1, 5, 1, 9, 1, 2, 2, 10, 10, 7, 2, 6, 10, 8, 7, 10, 10, 1, 7, 4, 6, 6, 2, 6, 8, 4, 5, 6, 7, 8, 3, 1, 8, 7, 7, 1, 9, 6, 1, 9, 6, 1, 9, 8, 3, 10, 6, 8, 3, 9, 2, 8, 4, 6, 6, 7, 4, 5, 1, 5, 7, 8, 4, 2, 8, 1, 7, 5, 3, 6, 6, 3, 2, 7, 7, 8, 1, 5, 1, 6, 10, 4, 3, 9, 7, 6, 4, 10, 3, 7, 2, 3, 1, 6, 10, 4, 10, 1, 5, 10, 5, 6, 2, 10, 1, 8, 8, 7, 6, 10, 2, 4, 7, 8, 8, 7, 9, 9, 2, 8, 6, 2, 1, 7, 2, 10, 3, 3, 5, 8, 8, 4, 8, 1, 8, 9, 7, 2, 3, 4, 5, 10, 2, 3, 6, 10, 7, 1, 8, 9, 9, 10, 9, 5, 9, 9, 10, 4, 4, 4, 7, 5, 1, 3, 3, 7, 1, 7, 6, 8, 6, 5, 1, 8, 9, 9, 8, 10, 6, 2, 7, 10, 8, 6, 2, 2, 7, 1, 10, 5, 4, 6, 9, 7, 6, 4, 2, 10, 4, 2, 7, 5, 1, 9, 5, 9, 2, 2, 7, 1, 2, 6, 2, 4, 10, 9, 8, 5, 5, 9, 9, 1, 1, 8, 4, 10, 6, 3, 3, 9, 3, 2, 4, 8, 9, 4, 4, 7, 3, 3, 10, 1, 1, 8, 3, 8, 9, 1, 5, 2, 5, 8, 5, 5, 4], \"Freq\": [0.9977119097127944, 0.9985563703021558, 0.9973519510421966, 0.9986373096150148, 0.9990641634830592, 0.9990105362333085, 0.9978407056897235, 0.9980206578802587, 0.9996045488401121, 0.9995384931469624, 0.9992034724326009, 0.9996954459092788, 0.9993690271008314, 0.9979160800327943, 0.9973619341989033, 0.9993231535627642, 0.9990910818704353, 0.9990449511420201, 0.9977038066136544, 0.998707833288312, 0.9969795687245666, 0.9990643211987705, 0.9994087906553181, 0.9978621459029867, 0.9993990254571549, 0.9982775841802121, 0.9996412559193835, 0.9993568564225807, 0.9990476927360106, 0.9986598096195829, 0.0009457005772912717, 0.9996880429479232, 0.9982201602800247, 0.9983468580246868, 0.9995497166549608, 0.9987610247942598, 0.9992389826333897, 0.9969711422748895, 0.9992247526038289, 0.9985623053766727, 0.9992829714794488, 0.9994640453253033, 0.9989703932833581, 0.998565813612602, 0.9987301060458904, 0.9993358848334978, 0.9997152615490146, 0.999534671952602, 0.9987495681883444, 0.9994653294817909, 0.9973289497913546, 0.9994423560068636, 0.9980632709612777, 0.9994137709172096, 0.9965917648191925, 0.9968950686240455, 0.9983140311696501, 0.9983531770758, 0.9983031068787245, 0.9976707358867483, 0.998716080333128, 0.9992423265491336, 0.9992202141315693, 0.9981746313307168, 0.9981755443581762, 0.9977793909356564, 0.9992790084005013, 0.9983143448405084, 0.9984717085669707, 0.9982016300700685, 0.9993910027625176, 0.9995419503106407, 0.999153640824228, 0.9991128366107028, 0.9972935117813827, 0.99879864540823, 0.9965672870523564, 0.99890370150869, 0.9993231191244613, 0.9988850232794809, 0.9992830103466082, 0.9972187183783578, 0.9993728495573312, 0.20688434263288266, 0.792549577292146, 0.8086671907177284, 0.19027463311005374, 0.9983678334021391, 0.9989150135232784, 0.9991234939088852, 0.9980347502145278, 0.9985209341399908, 0.9994646378993933, 0.9975193372819038, 0.9975501543629118, 0.9995729711400119, 0.9977772067407452, 0.9980461658492342, 0.9985223374130141, 0.998666568399479, 0.9990777602419884, 0.9988432084430182, 0.999682686857779, 0.997065550648653, 0.8399564664225281, 0.1597181242581825, 0.9984861298134647, 0.999400787257395, 0.9995225760930523, 0.22915617262544277, 0.7697015941773245, 0.9990570760885598, 0.9990343645740617, 0.9992865788315678, 0.9992541503545251, 0.9990474811988445, 0.9994449887538805, 0.9994132138149361, 0.9984160757129427, 0.9969647201660689, 0.9983545166920158, 0.9987922031540696, 0.9986765223234769, 0.9986001664118928, 0.9996643530300812, 0.9984459260282126, 0.9981164551677416, 0.9987649356130263, 0.998134895670671, 0.9996993368865816, 0.9985770421321666, 0.9979895650035918, 0.9966182596905387, 0.9990276428093368, 0.9977756615631236, 0.999047590438936, 0.9998733438581467, 0.9996522378980528, 0.9998082500734264, 0.9986186832258128, 0.9985088695131441, 0.9997778141278668, 0.9995099626031364, 0.998532842189408, 0.9977747775366754, 0.9988242921720096, 0.9993079185127002, 0.9984353685069705, 0.998586558588174, 0.9996664672342181, 0.9991335669610678, 0.9980529718469928, 0.9969612732391551, 0.9997656211922354, 0.998289064949216, 0.9969568261962871, 0.9982554081511014, 0.9983376723866133, 0.9978627616742944, 0.9984731236175796, 0.9984165184335073, 0.9995546150498875, 0.9985733902616012, 0.21022390347777467, 0.7895775732334406, 0.9977374099188162, 0.999025442691707, 0.997494861995502, 0.9990628506200315, 0.9988598125486088, 0.9989210505350854, 0.9993350413711707, 0.9997231049664804, 0.9985984517627596, 0.9977069715706831, 0.9995210780152927, 0.9989488340142828, 0.9981599448812702, 0.9991875193860159, 0.9995448360371246, 0.9987687175339423, 0.9984942429836172, 0.9985213086697904, 0.18315821096991516, 0.8167205033399723, 0.9982502611788382, 0.9986594040183465, 0.9977178153211179, 0.9988753795244585, 0.9988593614563668, 0.9992677336542867, 0.9982823498178968, 0.9995076268435938, 0.9998156792945093, 0.9993980917112755, 0.9998330485048464, 0.9991131283241962, 0.9991063042657141, 0.9991364110300341, 0.9981988385861585, 0.9987808405499754, 0.9982186052755085, 0.9990580668716619, 0.9990506569960924, 0.9992156549221486, 0.9995070717867963, 0.9978360725262394, 0.9975934745168572, 0.9994319080498264, 0.9993592370142556, 0.9995949615734829, 0.9977679150209148, 0.998858334531328, 0.9984957234100847, 0.9980730553983512, 0.9991154623402488, 0.9981863626536609, 0.9987393195016827, 0.1883458701964414, 0.11856870937157592, 0.16683561761133248, 0.44358337952950194, 0.08249968826849696, 0.998630947305999, 0.9993578178797632, 0.9985875824386132, 0.9992817818550426, 0.9981653387436599, 0.8553100989512451, 0.14354854807573345, 0.9993370533595788, 0.9992890769595358, 0.9980787151822347, 0.9980270795150594, 0.9988979442790785, 0.9989813302907052, 0.9996306390913661, 0.9993690187782017, 0.9990459724539278, 0.9988134106339983, 0.9996089314366142, 0.9989955913676222, 0.9995500152951917, 0.9979512255076731, 0.999677296778657, 0.9978688130301457, 0.9992077886304622, 0.997055392925432, 0.99852430817365, 0.9991638684879955, 0.9987991266793463, 0.9985364154907783, 0.9986646674826376, 0.99976739422706, 0.9984891772090927, 0.9991932555425761, 0.9989403730934213, 0.9979094918464163, 0.9997482844996081, 0.9980147676832928, 0.9974833773169975, 0.9990858414724397, 0.9992939827036188, 0.9976011856167929, 0.9984240653153434, 0.9987119357263485, 0.18116996694874477, 0.8188473082429142, 0.9987059036939321, 0.9977852431473202, 0.9987870562774172, 0.9977386694699242, 0.9987530495733932, 0.9995657535840838, 0.9997811974951194, 0.9995221497776264, 0.9975136021243947, 0.9993425331371456, 0.9987272570897223, 0.9982752355589687, 0.9989394428150286, 0.9992932548913502, 0.9983347295670854, 0.9989156599428602, 0.9994302478832122, 0.9974938326093763, 0.9981671855140322, 0.9996429587484918, 0.18042862174400692, 0.8192668039955148, 0.9982860294995431, 0.9991048819376617, 0.999357505213731, 0.3756426439823219, 0.624106336079259, 0.9988518543330464, 0.9976264281392676, 0.9984560547716222, 0.9991962622031143, 0.9990534555180612, 0.9988871519771434, 0.99790824012852, 0.9994159539883213, 0.9993004948690118, 0.9993445211822058, 0.9976107464823538, 0.9976918638176583, 0.997214228822302, 0.9998685169074408, 0.998531085759936, 0.9992284746295432, 0.9994583991521547, 0.998543326405246, 0.9982813108532205, 0.9980795983805211, 0.9990098786592747, 0.999578275072482, 0.9993082580069192, 0.9982825572479885, 0.9973121728265645, 0.7896933609844176, 0.2102663109429009, 0.9992999038383098, 0.9998875914671502, 0.9989491845672431, 0.19225927142274127, 0.6259816088503471, 0.18132509484893608, 0.8584537111957392, 0.14147203127634225, 0.9985826874099747, 0.9553669748051343, 0.04421917788379428, 0.9982435875361881, 0.9986060860372151, 0.9976661745787578], \"Term\": [\"able\", \"accept\", \"act\", \"actually\", \"african\", \"allow\", \"allowed\", \"also\", \"always\", \"america\", \"american\", \"americans\", \"another\", \"answer\", \"answers\", \"anti\", \"anyone\", \"anything\", \"around\", \"asian\", \"asians\", \"ask\", \"atheists\", \"average\", \"away\", \"back\", \"bad\", \"become\", \"believe\", \"best\", \"best\", \"better\", \"big\", \"bjp\", \"black\", \"black people\", \"blacks\", \"body\", \"boys\", \"british\", \"call\", \"called\", \"care\", \"change\", \"child\", \"children\", \"china\", \"chinese\", \"christian\", \"christians\", \"citizens\", \"claim\", \"clinton\", \"come\", \"common\", \"compared\", \"conservatives\", \"consider\", \"considered\", \"control\", \"could\", \"countries\", \"country\", \"culture\", \"daughter\", \"day\", \"democrats\", \"die\", \"different\", \"dog\", \"donald\", \"donald trump\", \"done\", \"earth\", \"east\", \"eat\", \"election\", \"english\", \"especially\", \"etc\", \"europe\", \"european\", \"europeans\", \"even\", \"even\", \"ever\", \"ever\", \"every\", \"everyone\", \"everything\", \"face\", \"fact\", \"fake\", \"family\", \"fat\", \"feel\", \"female\", \"feminists\", \"find\", \"first\", \"fuck\", \"full\", \"gay\", \"gender\", \"get\", \"get\", \"getting\", \"girl\", \"girls\", \"give\", \"give\", \"go\", \"god\", \"going\", \"good\", \"got\", \"government\", \"great\", \"gun\", \"guns\", \"guy\", \"guys\", \"happen\", \"hard\", \"hate\", \"help\", \"high\", \"hillary\", \"hindu\", \"hindus\", \"history\", \"hitler\", \"house\", \"human\", \"illegal\", \"immigrants\", \"india\", \"indian\", \"indians\", \"instead\", \"iq\", \"islam\", \"israel\", \"japanese\", \"jesus\", \"jewish\", \"jews\", \"keep\", \"kids\", \"kill\", \"killed\", \"killing\", \"kind\", \"know\", \"korea\", \"last\", \"law\", \"left\", \"less\", \"let\", \"liberal\", \"liberals\", \"life\", \"like\", \"like\", \"likely\", \"live\", \"living\", \"long\", \"look\", \"looking\", \"lot\", \"love\", \"made\", \"majority\", \"make\", \"makes\", \"male\", \"man\", \"many\", \"marry\", \"mean\", \"media\", \"men\", \"men\", \"mental\", \"middle\", \"military\", \"modern\", \"modi\", \"mom\", \"money\", \"mother\", \"much\", \"muslim\", \"muslims\", \"name\", \"nation\", \"need\", \"never\", \"new\", \"news\", \"non\", \"normal\", \"north\", \"obama\", \"often\", \"ok\", \"okay\", \"old\", \"one\", \"others\", \"pakistan\", \"pakistani\", \"parents\", \"part\", \"party\", \"penis\", \"people\", \"people\", \"people\", \"people\", \"people\", \"person\", \"police\", \"political\", \"poor\", \"population\", \"possible\", \"possible\", \"president\", \"president trump\", \"public\", \"put\", \"question\", \"questions\", \"quora\", \"race\", \"racist\", \"rape\", \"real\", \"realize\", \"really\", \"reason\", \"religion\", \"religious\", \"republicans\", \"respect\", \"rich\", \"right\", \"rights\", \"russia\", \"russian\", \"say\", \"school\", \"see\", \"seem\", \"self\", \"sex\", \"sexual\", \"show\", \"since\", \"sister\", \"small\", \"social\", \"society\", \"someone\", \"someone\", \"something\", \"son\", \"south\", \"start\", \"state\", \"states\", \"still\", \"stop\", \"students\", \"stupid\", \"support\", \"supporters\", \"take\", \"taking\", \"tell\", \"terrorist\", \"terrorists\", \"thing\", \"things\", \"think\", \"time\", \"time\", \"today\", \"towards\", \"true\", \"trump\", \"trump\", \"try\", \"trying\", \"two\", \"uk\", \"understand\", \"united\", \"united states\", \"us\", \"usa\", \"use\", \"used\", \"using\", \"vote\", \"want\", \"wants\", \"war\", \"way\", \"wear\", \"well\", \"west\", \"western\", \"white\", \"white people\", \"wife\", \"win\", \"without\", \"without\", \"woman\", \"women\", \"work\", \"world\", \"world\", \"world\", \"would\", \"would\", \"wrong\", \"year\", \"year\", \"year old\", \"years\", \"yet\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [5, 3, 6, 7, 9, 2, 4, 8, 10, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1837258728076321289394716\", ldavis_el1837258728076321289394716_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1837258728076321289394716\", ldavis_el1837258728076321289394716_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1837258728076321289394716\", ldavis_el1837258728076321289394716_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=                x           y  topics  cluster       Freq\n",
       "topic                                                    \n",
       "4     -117.727615 -132.594223       1        1  10.938934\n",
       "2      113.927292   95.090324       2        1  10.268970\n",
       "5      253.289703 -278.334167       3        1  10.175497\n",
       "6     -258.650452  242.431931       4        1  10.149837\n",
       "8     -375.552368 -389.463348       5        1   9.960029\n",
       "1       -5.128246 -520.291626       6        1   9.931058\n",
       "3      499.706390  -24.195478       7        1   9.875112\n",
       "7        4.754779  480.681854       8        1   9.743104\n",
       "9     -501.411987  -16.822897       9        1   9.505497\n",
       "0      375.205811  348.394379      10        1   9.451962, topic_info=     Category         Freq        Term        Total  loglift  logprob\n",
       "term                                                                 \n",
       "484   Default  4493.000000       women  4493.000000  30.0000  30.0000\n",
       "256   Default  4442.000000        like  4442.000000  29.0000  29.0000\n",
       "304   Default  3269.000000     muslims  3269.000000  28.0000  28.0000\n",
       "362   Default  3153.000000       quora  3153.000000  27.0000  27.0000\n",
       "446   Default  4411.000000       trump  4411.000000  26.0000  26.0000\n",
       "215   Default  3033.000000       india  3033.000000  25.0000  25.0000\n",
       "337   Default  7624.000000      people  7624.000000  24.0000  24.0000\n",
       "287   Default  3270.000000         men  3270.000000  23.0000  23.0000\n",
       "276   Default  2673.000000        many  2673.000000  22.0000  22.0000\n",
       "478   Default  2689.000000       white  2689.000000  21.0000  21.0000\n",
       "438   Default  2702.000000       think  2702.000000  20.0000  20.0000\n",
       "216   Default  2606.000000      indian  2606.000000  19.0000  19.0000\n",
       "198   Default  2485.000000        hate  2485.000000  18.0000  18.0000\n",
       "396   Default  2400.000000         sex  2400.000000  17.0000  17.0000\n",
       "14    Default  2446.000000   americans  2446.000000  16.0000  16.0000\n",
       "176   Default  2338.000000       girls  2338.000000  15.0000  15.0000\n",
       "217   Default  2327.000000     indians  2327.000000  14.0000  14.0000\n",
       "70    Default  2206.000000     chinese  2206.000000  13.0000  13.0000\n",
       "490   Default  2806.000000       would  2806.000000  12.0000  12.0000\n",
       "172   Default  2610.000000         get  2610.000000  11.0000  11.0000\n",
       "45    Default  2117.000000       black  2117.000000  10.0000  10.0000\n",
       "466   Default  2091.000000        want  2091.000000   9.0000   9.0000\n",
       "253   Default  1965.000000    liberals  1965.000000   8.0000   8.0000\n",
       "301   Default  1976.000000        much  1976.000000   7.0000   7.0000\n",
       "458   Default  2067.000000          us  2067.000000   6.0000   6.0000\n",
       "303   Default  1908.000000      muslim  1908.000000   5.0000   5.0000\n",
       "271   Default  1623.000000        make  1623.000000   4.0000   4.0000\n",
       "323   Default  1643.000000         one  1643.000000   3.0000   3.0000\n",
       "150   Default  1630.000000        feel  1630.000000   2.0000   2.0000\n",
       "182   Default  1597.000000        good  1597.000000   1.0000   1.0000\n",
       "...       ...          ...         ...          ...      ...      ...\n",
       "271   Topic10  1623.009201        make  1623.777663   2.3585  -2.9370\n",
       "424   Topic10   803.254872     support   804.023315   2.3580  -3.6404\n",
       "119   Topic10   716.092822         eat   716.861204   2.3579  -3.7552\n",
       "402   Topic10   687.717712      sister   688.486083   2.3578  -3.7957\n",
       "34    Topic10   665.378936        back   666.147383   2.3578  -3.8287\n",
       "136   Topic10   655.944093    everyone   656.712524   2.3578  -3.8430\n",
       "294   Topic10   649.707955         mom   650.476322   2.3578  -3.8525\n",
       "363   Topic10   615.620504        race   616.388930   2.3577  -3.9064\n",
       "4     Topic10   607.059849    actually   607.828282   2.3577  -3.9204\n",
       "274   Topic10   598.333993        male   599.102381   2.3577  -3.9349\n",
       "345   Topic10   559.634063        poor   560.402491   2.3576  -4.0017\n",
       "152   Topic10   555.318120   feminists   556.086501   2.3576  -4.0095\n",
       "151   Topic10   553.463558      female   554.231943   2.3576  -4.0128\n",
       "101   Topic10   546.446723         day   547.215151   2.3575  -4.0256\n",
       "411   Topic10   535.419127         son   536.187525   2.3575  -4.0460\n",
       "76    Topic10   518.521139       claim   519.289579   2.3575  -4.0781\n",
       "261   Topic10   517.717480        long   518.485899   2.3575  -4.0796\n",
       "23    Topic10   488.354693      around   489.123121   2.3574  -4.1380\n",
       "447   Topic10   479.783300         try   480.551743   2.3573  -4.1557\n",
       "281   Topic10   465.806078       marry   466.574485   2.3573  -4.1853\n",
       "146   Topic10   465.387971      family   466.156377   2.3573  -4.1862\n",
       "201   Topic10   455.090214        high   455.858630   2.3573  -4.2085\n",
       "358   Topic10   439.076667      public   439.845068   2.3572  -4.2444\n",
       "115   Topic10   433.599203        done   434.367631   2.3572  -4.2569\n",
       "462   Topic10   416.193992       using   416.962406   2.3571  -4.2979\n",
       "87    Topic10   411.911201    consider   412.679610   2.3571  -4.3082\n",
       "319   Topic10   411.122894       often   411.891303   2.3571  -4.3101\n",
       "137   Topic10   393.577225  everything   394.345646   2.3570  -4.3538\n",
       "287   Topic10  2670.512907         men  3270.396652   2.1563  -2.4390\n",
       "337   Topic10   629.235249      people  7624.271233  -0.1356  -3.8845\n",
       "\n",
       "[355 rows x 6 columns], token_table=      Topic      Freq          Term\n",
       "term                               \n",
       "0         7  0.997712          able\n",
       "1         5  0.998556        accept\n",
       "3         2  0.997352           act\n",
       "4        10  0.998637      actually\n",
       "5         5  0.999064       african\n",
       "7         9  0.999011         allow\n",
       "8         2  0.997841       allowed\n",
       "10        4  0.998021          also\n",
       "11        8  0.999605        always\n",
       "12        3  0.999538       america\n",
       "13        2  0.999203      american\n",
       "14        6  0.999695     americans\n",
       "16        8  0.999369       another\n",
       "17        5  0.997916        answer\n",
       "18        9  0.997362       answers\n",
       "19        9  0.999323          anti\n",
       "20        3  0.999091        anyone\n",
       "21        9  0.999045      anything\n",
       "23       10  0.997704        around\n",
       "24        2  0.998708         asian\n",
       "25        2  0.996980        asians\n",
       "26        5  0.999064           ask\n",
       "29        3  0.999409      atheists\n",
       "32        7  0.997862       average\n",
       "33        3  0.999399          away\n",
       "34       10  0.998278          back\n",
       "35        5  0.999641           bad\n",
       "37        3  0.999357        become\n",
       "39        1  0.999048       believe\n",
       "40        3  0.998660          best\n",
       "...     ...       ...           ...\n",
       "462      10  0.997692         using\n",
       "465       6  0.997214          vote\n",
       "466       3  0.999869          want\n",
       "467       3  0.998531         wants\n",
       "468       9  0.999228           war\n",
       "471       3  0.999458           way\n",
       "473       2  0.998543          wear\n",
       "475       4  0.998281          well\n",
       "476       8  0.998080          west\n",
       "477       9  0.999010       western\n",
       "478       4  0.999578         white\n",
       "479       4  0.999308  white people\n",
       "480       7  0.998283          wife\n",
       "481       3  0.997312           win\n",
       "482       3  0.789693       without\n",
       "482      10  0.210266       without\n",
       "483       1  0.999300         woman\n",
       "484       1  0.999888         women\n",
       "486       8  0.998949          work\n",
       "488       3  0.192259         world\n",
       "488       8  0.625982         world\n",
       "488       9  0.181325         world\n",
       "490       1  0.858454         would\n",
       "490       5  0.141472         would\n",
       "494       2  0.998583         wrong\n",
       "495       5  0.955367          year\n",
       "495       8  0.044219          year\n",
       "496       5  0.998244      year old\n",
       "497       5  0.998606         years\n",
       "498       4  0.997666           yet\n",
       "\n",
       "[336 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[5, 3, 6, 7, 9, 2, 4, 8, 10, 1])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.sklearn\n",
    " \n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda, X_tf_lda, vectorizer_lda, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "76b030e9f0ce61db12ef3bbd740a5e53d135d13d"
   },
   "outputs": [],
   "source": [
    "X_lda = lda.transform(X_tf_lda)\n",
    "y_clf = train_df['target']\n",
    "X_lda_train, X_lda_test, y_lda_train, y_lda_test = train_test_split(X_lda, y_clf, test_size=0.2, random_state=0, stratify=y_clf)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_tf, y_clf, test_size=0.2, random_state=0, stratify=y_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "341245e9729ea75108eb69fc8babd9f400835104"
   },
   "outputs": [],
   "source": [
    "# y_svm = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f1b830134a318c3ce2a29d5d403bc97b8dd8c7a8"
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler().fit(X_train)\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "9ec78ff20cf404d9612d7a09d9d73e4034bd70d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=9, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# clf = SVC(C=10, kernel='linear', random_state=9)\n",
    "# clf = RandomForestClassifier(n_estimators=100, random_state=9)\n",
    "# clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "clf2 = RandomForestClassifier(n_estimators=100, random_state=9)\n",
    "clf2.fit(X_lda_train, y_lda_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "0e57a7451ed97ef5f6c6d402dcc27c6375db4dfe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[118735,   3779],\n",
       "       [ 13335,  10926]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p_pred_y = clf.predict(X_test)\n",
    "# confusion_matrix(y_test, p_pred_y)\n",
    "\n",
    "p_pred_y_lda = clf2.predict(X_lda_test)\n",
    "confusion_matrix(y_lda_test, p_pred_y_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "33f2ffde33db03d57d4635bc1d83f5263228a086"
   },
   "outputs": [],
   "source": [
    "def print_cm(y_test, p_pred_y):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, p_pred_y).ravel()\n",
    "\n",
    "    acc = (tp + tn) / (tn + fp + fn + tp)\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f1 = 2 * prec * rec / (prec + rec)\n",
    "\n",
    "    print(\"accuracy\", acc)\n",
    "    print(\"precision\", prec)\n",
    "    print(\"recall\", rec)\n",
    "    print(\"f1\", f1)\n",
    "    print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "2106f7879d4e4b4d42a71b833369c01ca4c7cb7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8833997615397717\n",
      "precision 0.7430125807548453\n",
      "recall 0.45035241746012117\n",
      "f1 0.5607965919006312\n",
      "118735 3779 13335 10926\n"
     ]
    }
   ],
   "source": [
    "# print_cm(y_test, p_pred_y)\n",
    "print_cm(y_lda_test, p_pred_y_lda)\n",
    "\n",
    "# print_cm(p_pred_y, p_pred_y_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4f1c1c539e81445e09c9f0c3748de5e6ec7c6fe4"
   },
   "outputs": [],
   "source": [
    "X_init_lda_test = lda.transform(vectorizer_lda.transform(test_df['p_txtcn']))\n",
    "# X_init_lda_test = scaler.transform(X_init_lda_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0ca47320fd4d8188902275695df03c8e20c95b64"
   },
   "outputs": [],
   "source": [
    "y_pred_final = clf.predict(vectorizer.transform(test_df['p_txtcn']))\n",
    "y_pred_final_lda = clf2.predict(X_init_lda_test)\n",
    "\n",
    "y_1 = np.maximum(y_pred_final, y_pred_final_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "38788e774292beabc8d1c468e65caf20c9459910"
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n",
    "sub['prediction'] = y_1\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "025bb9348c0c3561c08a237b4176e7502b5ce28a"
   },
   "outputs": [],
   "source": [
    "# X_lda[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ea1105b941266e84642382f22c942f6a695b2f4"
   },
   "outputs": [],
   "source": [
    "# X_lda_df = pd.DataFrame(X_lda)\n",
    "# X_lda_df[\"idxmax\"] = X_lda_df.idxmax(axis=1)\n",
    "# X_lda_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b2f52f01b288533a0be1b1769a361ccd81803c3e"
   },
   "outputs": [],
   "source": [
    "# y_true = train_df[\"target\"]\n",
    "# y_lda = X_lda_df[\"idxmax\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29ab04b1f841d93d9fd38fc68832a14e1c460e92"
   },
   "outputs": [],
   "source": [
    "# print(confusion_matrix(y_true, y_lda)[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d154e0cce3e2571ac647779024a42f78b7ff55bd"
   },
   "source": [
    "#### Trying Deep Learning with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f02899931638b7e0defb426c045a94f36e6ec5aa"
   },
   "outputs": [],
   "source": [
    "def fit_get_sequences(df, vocab_size):\n",
    "    tokenizer = Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(df['p_txt'])\n",
    "    sequences = tokenizer.texts_to_sequences(df['p_txt'])\n",
    "    return sequences, tokenizer, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9bb49ea62415ada2045514c07241e388a8489949"
   },
   "outputs": [],
   "source": [
    "def get_sequences(df, tokenizer):\n",
    "    return tokenizer.texts_to_sequences(df['p_txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "794589d6f3e18fb4820355909e5eec559cd9d2e7"
   },
   "outputs": [],
   "source": [
    "# vocab_size = 1000\n",
    "# sequences, tokenizer, vocab_size = fit_get_sequences(train_df, vocab_size)\n",
    "# word_index = tokenizer.word_index\n",
    "# print('Found %s unique tokens.' % len(word_index))\n",
    "# avg = sum(map(len, sequences)) / len(sequences)\n",
    "# std = np.sqrt(sum(map(lambda x: (len(x) - avg) ** 2, sequences)) / len(sequences))\n",
    "# print(\"Tokens avg {} and std {}\".format(avg, std))\n",
    "\n",
    "# max_length = 100\n",
    "# X = pad_sequences(sequences, maxlen=max_length)\n",
    "# X_init_test = pad_sequences(get_sequences(test_df, tokenizer), maxlen=max_length)\n",
    "\n",
    "# y = to_categorical(np.asarray(train_df['target']))\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c7b21b54e9a25702b44650424b0e9f71151b20d"
   },
   "outputs": [],
   "source": [
    "# y = to_categorical(np.asarray(train_df['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d48f305db9ad82cb59148ca481aac02a53d03994"
   },
   "outputs": [],
   "source": [
    "# print('Shape of data:', X.shape)\n",
    "# print('Shape of labels:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1868fd3037449e63752c14b183453c678ba4ecf0"
   },
   "outputs": [],
   "source": [
    "def generate_embeddings(vocab_size, word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    embeddings_index = {}\n",
    "    f = open(EMBEDDING_FILE)\n",
    "    # In the dataset, each line represents a new word embedding\n",
    "    # The line starts with the word and the embedding values follow\n",
    "#     for line in f:\n",
    "#         values = line.split()\n",
    "#         word = values[0]\n",
    "#         embedding = np.asarray(values[1:], dtype='float32')\n",
    "#         embeddings_index[word] = embedding \n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in f if o.split(\" \")[0] in word_index)\n",
    "    f.close()\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean = all_embs.mean()\n",
    "    emb_std = all_embs.std()\n",
    "    print(emb_mean, emb_std)\n",
    "    embedding_dim = 300\n",
    "    nb_words = min(vocab_size, len(word_index))  # How many words are there actually\n",
    "    # Create a random matrix with the same mean and std as the embeddings\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_dim))\n",
    "    # The vectors need to be in the same position as their index.\n",
    "    # Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "    # Loop over all words in the word index\n",
    "    for word, i in word_index.items():\n",
    "        # If we are above the amount of words we want to use we do nothing\n",
    "        if i >= vocab_size:\n",
    "            break\n",
    "        # Get the embedding vector for the word\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        # If there is an embedding vector, put it in the embedding matrix\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_dim, embedding_matrix    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "56800892d2ddd58a63f06127c9c49a9447369deb"
   },
   "outputs": [],
   "source": [
    "# embedding_dim, embedding_matrix = generate_embeddings(vocab_size, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc076eee75a3cbd2c40d4a65e23a1ddcd7f034f3"
   },
   "outputs": [],
   "source": [
    "def generate_model(embedding_dim, embedding_matrix, max_length, vocab_size, ifembed=True):\n",
    "    model = Sequential()\n",
    "    # model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    if ifembed:\n",
    "        model.add(\n",
    "            Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embedding_matrix], trainable=False))\n",
    "        model.add(LSTM(32, return_sequences=True))\n",
    "    else:\n",
    "        model.add(LSTM(32, return_sequences=True, input_shape=(max_length)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(32, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                  metrics=[metrics.mae, metrics.categorical_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "905ccc24c9660d6790e7ce28ee42e5355fb06b10"
   },
   "outputs": [],
   "source": [
    "# model = generate_model(embedding_dim, embedding_matrix, max_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "135104fa288d21fb9c6e1665bddb102308227627"
   },
   "outputs": [],
   "source": [
    "# batch_size = 1000\n",
    "# model.fit(X_train, y_train, batch_size=batch_size, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ca8b6a659113073b37a789ae4ccf5c87eada9541"
   },
   "outputs": [],
   "source": [
    "# y_svm = train_df['target']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_svm, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "# clf = SVC(C=100, kernel='linear', random_state=9)\n",
    "# clf.fit(X_train, y_train)\n",
    "# p_pred_y = clf.predict(X_test)\n",
    "\n",
    "# confusion_matrix(y_test, p_pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42ff8dd57cedc4b1227e4f92a367c4085430961f"
   },
   "outputs": [],
   "source": [
    "def c_matrix(y_true, y_pred, num_classes=2):\n",
    "    cm = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    print(cm)\n",
    "    cm_np = np.asarray(cm)\n",
    "    TP = np.diag(cm_np)\n",
    "    FP = np.sum(cm, axis=0) - TP\n",
    "    FN = np.sum(cm, axis=1) - TP\n",
    "    TN = []\n",
    "    for i in range(num_classes):\n",
    "        temp = np.delete(cm, i, 0)\n",
    "        temp = np.delete(temp, i, 1)\n",
    "        TN.append(sum(sum(temp)))\n",
    "    prec = TP / (TP + FP)\n",
    "    rec = TP / (TP + FN)\n",
    "    acc = (TP + TN) / (TP + FP + TN + FN)\n",
    "    f1 = 2 * prec * rec / (prec + rec)\n",
    "    print(\"precision\", prec)\n",
    "    print(\"recall\", rec)\n",
    "    print(\"accuracy\", acc)\n",
    "    print(\"f1\", f1)\n",
    "    return prec, rec, acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3777868003dd11608e541bce9436b5da15b21d0b"
   },
   "outputs": [],
   "source": [
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# c_matrix(y_test, y_pred, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7d8b6150b53443b71ae5caba24980fa5e6cbe412"
   },
   "outputs": [],
   "source": [
    "# y_pred_final = model.predict(X_init_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53ebd4ab8be7625789c8d6315a01ddd9cb6c09b3"
   },
   "outputs": [],
   "source": [
    "# y_1 = y_pred_final.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a943cb49d6b8ed68591ca6e94503a5e6163bc16b"
   },
   "outputs": [],
   "source": [
    "# sub = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n",
    "# sub['prediction'] = y_1\n",
    "# sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bb9120991ee58fedae001028ccf22680380f12aa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
